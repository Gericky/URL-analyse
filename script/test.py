"""
æ¨¡å‹è¯„ä¼°è„šæœ¬ - éªŒè¯ LoRA å¾®è°ƒæ•ˆæœ
"""
import torch
print(torch.__version__)  # åº”ç±»ä¼¼ 2.6.0.dev...
print(torch.cuda.is_available())
import os
import json
import torch
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ========================
# é…ç½®
# ========================

BASE_MODEL = "d:/code/URL-analyse/Qwen3-0.6B"
# LORA_PATH = "d:/code/URL-analyse/script/output/lora_online"
# TEST_DATA = "d:/code/URL-analyse/script/data/finetune_online/test/val.jsonl"
LORA_PATH = "d:/code/URL-analyse/script/output/lora_online_v2"
TEST_DATA = "d:/code/URL-analyse/script/data/finetune_online/raw/test/val.jsonl"
# ========================
# åŠ è½½æ¨¡å‹
# ========================

print("ğŸš€ åŠ è½½æ¨¡å‹...")

# ä»æœ¬åœ°åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL,  # ä» base model åŠ è½½
    trust_remote_code=True,
    local_files_only=True
)

# ä»æœ¬åœ°åŠ è½½ base model
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    local_files_only=True
)

# åŠ è½½ LoRA æƒé‡
model = PeftModel.from_pretrained(
    base_model, 
    LORA_PATH,
    local_files_only=True
)
model.eval()

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ========================
# æ¨ç†å‡½æ•°
# ========================

PROMPT_TEMPLATE = """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªURLå®‰å…¨æ£€æµ‹ç³»ç»Ÿ,ä¸“é—¨è¯†åˆ«æ¶æ„URLã€‚è¯·åˆ¤æ–­ç»™å®šURLæ˜¯å¦å­˜åœ¨å¨èƒã€‚
è¾“å‡ºæ ¼å¼è¦æ±‚: 
- å¦‚æœURLå®‰å…¨,è¾“å‡º: 0|benign
- å¦‚æœURLå­˜åœ¨å¨èƒ,è¾“å‡º: 1|å¨èƒç±»å‹ (å¦‚ phishing, malware, defacement ç­‰)
<|im_end|>
<|im_start|>user
{instruction}
è¾“å…¥URL: {input}<|im_end|>
<|im_start|>assistant
"""

def predict(url, instruction="åˆ¤æ–­ä»¥ä¸‹URLæ˜¯å¦å­˜åœ¨å®‰å…¨å¨èƒ"):
    """å¯¹å•ä¸ªURLè¿›è¡Œé¢„æµ‹"""
    
    prompt = PROMPT_TEMPLATE.format(
        instruction=instruction,
        input=url
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            do_sample=False,
            # temperature=1.0,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
            # enable_thinking=False
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # æå– assistant çš„è¾“å‡º
    if "<|im_start|>assistant" in result:
        prediction = result.split("<|im_start|>assistant")[-1]
        if "<|im_end|>" in prediction:
            prediction = prediction.split("<|im_end|>")[0]
        prediction = prediction.strip()
    else:
        prediction = "ERROR"
    
    return prediction

# ========================
# æ‰¹é‡è¯„ä¼°
# ========================
from sklearn.metrics import classification_report

def evaluate_on_test_set(test_file, batch_size=1):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆäºŒåˆ†ç±»+å¤šç±»åˆ«F1ç»Ÿè®¡ï¼‰"""

    print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return None, None

    dataset = load_dataset("json", data_files={"test": test_file})
    test_data = dataset["test"]
    total_samples = len(test_data)
    print(f"âœ… æµ‹è¯•æ ·æœ¬æ•°: {total_samples}")

    results = []
    total, correct_binary, correct_full = 0, 0, 0
    y_true, y_pred = [], []

    print(f"\nğŸ” å¼€å§‹è¯„ä¼° (batch_size={batch_size})...")

    with tqdm(total=total_samples, desc="è¯„ä¼°è¿›åº¦") as pbar:
        for i in range(0, total_samples, batch_size):
            end_idx = min(i + batch_size, total_samples)
            batch_items = [test_data[j] for j in range(i, end_idx)]

            for item in batch_items:
                url = item["input"]
                ground_truth = item["output"]
                instruction = item.get("instruction", "åˆ¤æ–­ä»¥ä¸‹URLæ˜¯å¦å­˜åœ¨å®‰å…¨å¨èƒ")

                prediction = predict(url, instruction)

                # === è®°å½•åŸå§‹è¾“å‡º ===
                gt_binary = ground_truth.split("|")[0]
                gt_full = ground_truth.strip()
                pred_binary = prediction.split("|")[0].strip()
                pred_full = prediction.strip()

                # è®°å½•äºŒåˆ†ç±»æ­£ç¡®æ€§
                if gt_binary == pred_binary:
                    correct_binary += 1

                # è®°å½•å®Œå…¨åŒ¹é…ï¼ˆåŒ…å«æ”»å‡»ç±»å‹ï¼‰
                if gt_full == pred_full:
                    correct_full += 1

                y_true.append(gt_full)
                y_pred.append(pred_full)

                results.append({
                    "url": url,
                    "ground_truth": ground_truth,
                    "prediction": prediction,
                    "binary_correct": (gt_binary == pred_binary),
                    "full_correct": (gt_full == pred_full)
                })
                total += 1

            pbar.update(len(batch_items))

    # ======= æŒ‡æ ‡ç»Ÿè®¡ =======
    acc_binary = correct_binary / total if total else 0
    acc_full = correct_full / total if total else 0

    print("\n" + "=" * 70)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("=" * 70)
    print(f"æ€»ä½“å‡†ç¡®ç‡ï¼ˆä»…çœ‹0/1ï¼‰: {acc_binary:.2%}")
    print(f"å®Œæ•´åŒ¹é…å‡†ç¡®ç‡ï¼ˆå«æ”»å‡»ç±»å‹ï¼‰: {acc_full:.2%}")

    # ======= åˆ†ç±»æŠ¥å‘Šï¼ˆF1/Precision/Recallï¼‰ =======
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼ˆæŒ‰å®Œæ•´æ ‡ç­¾ï¼‰:")
    print(classification_report(y_true, y_pred, digits=3, zero_division=0))

    # ======= é”™è¯¯æ¡ˆä¾‹å±•ç¤º =======
    print("\n" + "=" * 70)
    print("âŒ é”™è¯¯é¢„æµ‹æ¡ˆä¾‹ï¼ˆå‰10ä¸ªï¼‰")
    print("=" * 70)
    wrongs = [r for r in results if not r["binary_correct"] or not r["full_correct"]]
    for i, case in enumerate(wrongs[:10], 1):
        print(f"\n{i}. URL: {case['url'][:80]}")
        print(f"   çœŸå€¼: {case['ground_truth']}")
        print(f"   é¢„æµ‹: {case['prediction']}")

    # ======= ä¿å­˜è¾“å‡º =======
    output_dir = "d:/code/URL-analyse/script/output"
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "evaluation_results_detailed.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ æ¨¡å‹è¾“å‡ºå·²ä¿å­˜åˆ°: {output_file}")
    return acc_full, results

# ========================
# äº¤äº’å¼æµ‹è¯•
# ========================

def interactive_test():
    """äº¤äº’å¼æµ‹è¯•æ¨¡å¼"""

    print("\n" + "=" * 70)
    print("ğŸ¯ äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
    print("=" * 70)
    print("è¾“å…¥ URL è¿›è¡Œæ£€æµ‹,è¾“å…¥ 'quit' é€€å‡º")

    while True:
        url = input("\nè¯·è¾“å…¥URL: ").strip()

        if url.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ é€€å‡ºæµ‹è¯•")
            break

        if not url:
            continue

        prediction = predict(url)
        print(f"ğŸ” æ£€æµ‹ç»“æœ: {prediction}")

# ========================
# ä¸»å‡½æ•°
# ========================

def main():
    evaluate_on_test_set(TEST_DATA)

if __name__ == "__main__":
    main()