"""
åœ¨çº¿æ£€æµ‹æ•°æ®å‡†å¤‡è„šæœ¬ (LoRA-online)
ç”Ÿæˆç®€æ´æ ‡ç­¾æ ¼å¼: 0|Benign, 1|SQLi, 1|XSS ç­‰
"""
import os
import sys
import json
import random
import re
from collections import defaultdict, Counter
from typing import List, Tuple, Dict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ========== ç»Ÿä¸€æ ‡ç­¾ä½“ç³» ==========
STANDARD_LABELS = {
    'Benign': '0|Benign',
    'SQLi': '1|SQLi',
    'XSS': '1|XSS',
    'LFI': '1|LFI',           # Local File Inclusion / Path Traversal
    'RCE': '1|RCE',           # Remote Code Execution
    'CMDi': '1|CMDi',         # Command Injection

}

# ========== æ•°æ®æºæ˜ å°„é…ç½® ==========
DATA_SOURCE_CONFIG = {
    'CCF-BDCI2022': {
        'base_dir': '../data/processed/CCF-BDCI2022/total',
        'files': {
            'all_normal.txt': 'Benign',
            'sql_injection.txt': 'SQLi',
            'xss.txt': 'XSS',
            'path_traversal.txt': 'LFI',
            'remote_code_execution.txt': 'RCE',
            'command_execution.txt': 'CMDi'
        }
    },
    'WAF-github': {
        'base_dir': '../data/processed/WAF-github/total',
        'files': {
            'normal_urls.txt': 'Benign',
            'sqli_urls.txt': 'SQLi',
            'xss_urls.txt': 'XSS'
        }
    },
    'CSIC-2010': {
        'base_dir': '../data/processed/CSIC-2010/total',
        'files': {
            'normal_urls.txt': 'Benign',
            # attack_urls.txt æ— ç»†åˆ†ç±»å‹ï¼Œæš‚ä¸ä½¿ç”¨
        }
    }
}

# ========== ç›®æ ‡æ ·æœ¬æ•°é…ç½® ==========
TARGET_SAMPLES_PER_CLASS = {
    'Benign': 10000,
    'SQLi': 10000,
    'XSS': 8000,
    'LFI': 5000,
    'RCE': 5000,
    'CMDi': 5000,
    'PathTraversal': 5000
}


def validate_url(url: str) -> bool:
    """
    URL æœ‰æ•ˆæ€§æ£€æŸ¥
    
    æ£€æŸ¥é¡¹:
    1. éç©º
    2. åŒ…å«åŸºæœ¬URLç‰¹å¾ (/, ?, &)
    3. é•¿åº¦åˆç† (3-2048)
    4. å¯ç¼–ç ä¸ºUTF-8
    """
    # 1. éç©ºæ£€æŸ¥
    if not url or url.isspace():
        return False
    
    # 2. å»é™¤é¦–å°¾ç©ºç™½
    url = url.strip()
    
    # 3. åŸºæœ¬ç»“æ„æ£€æŸ¥
    if not any(char in url for char in ['/', '?', '&', '=']):
        return False
    
    # 4. é•¿åº¦æ£€æŸ¥
    if len(url) < 3 or len(url) > 2048:
        return False
    
    # 5. ç¼–ç æ£€æŸ¥
    try:
        url.encode('utf-8')
    except (UnicodeEncodeError, UnicodeDecodeError):
        return False
    
    return True


def load_urls_from_file(filepath: str, label: str, source_name: str) -> List[Dict]:
    """
    ä»æ–‡ä»¶åŠ è½½URLå¹¶æ ‡æ³¨æ ‡ç­¾
    
    Returns:
        List[Dict]: [{'url': str, 'label': str, 'standard_label': str, 'source': str}, ...]
    """
    samples = []
    error_count = 0
    
    if not os.path.exists(filepath):
        print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return samples
    
    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                continue
            
            # æå–URLï¼ˆå¤„ç† URL\tå‚æ•° æ ¼å¼ï¼‰
            url = line.split('\t')[0] if '\t' in line else line
            
            # URLéªŒè¯
            if not validate_url(url):
                error_count += 1
                continue
            
            # è·å–æ ‡å‡†æ ‡ç­¾
            standard_label = STANDARD_LABELS.get(label)
            if not standard_label:
                print(f"  âŒ æ— æ³•æ˜ å°„æ ‡ç­¾: {label} (æ–‡ä»¶: {filepath}, è¡Œ: {line_num})")
                error_count += 1
                continue
            
            samples.append({
                'url': url,
                'label': label,
                'standard_label': standard_label,
                'source': source_name,
                'file': os.path.basename(filepath)
            })
    
    if error_count > 0:
        print(f"  âš ï¸  è·³è¿‡ {error_count} æ¡æ— æ•ˆæ ·æœ¬")
    
    return samples


def deduplicate_samples(samples: List[Dict]) -> List[Dict]:
    """
    å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„æ ·æœ¬ï¼‰
    """
    seen_urls = set()
    unique_samples = []
    
    for sample in samples:
        url = sample['url']
        if url not in seen_urls:
            seen_urls.add(url)
            unique_samples.append(sample)
    
    duplicate_count = len(samples) - len(unique_samples)
    if duplicate_count > 0:
        print(f"  ğŸ”„ å»é‡: ç§»é™¤ {duplicate_count} æ¡é‡å¤æ ·æœ¬")
    
    return unique_samples


def balance_samples(samples_by_label: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
    """
    æ•°æ®å¹³è¡¡å¤„ç†
    
    ç­–ç•¥:
    - æ ·æœ¬ä¸è¶³: é‡å¤é‡‡æ ·ï¼ˆè¿‡é‡‡æ ·ï¼‰
    - æ ·æœ¬è¿‡å¤š: éšæœºä¸‹é‡‡æ ·
    """
    balanced = {}
    
    for label, samples in samples_by_label.items():
        target_count = TARGET_SAMPLES_PER_CLASS.get(label, 5000)
        current_count = len(samples)
        
        if current_count == 0:
            print(f"  âš ï¸  {label}: æ— æ ·æœ¬ï¼Œè·³è¿‡")
            continue
        
        if current_count < target_count:
            # è¿‡é‡‡æ ·
            sampled = random.choices(samples, k=target_count)
            print(f"  ğŸ“ˆ {label:15s}: {current_count:6d} -> {len(sampled):6d} (è¿‡é‡‡æ ·)")
        elif current_count > target_count:
            # ä¸‹é‡‡æ ·
            sampled = random.sample(samples, k=target_count)
            print(f"  ğŸ“‰ {label:15s}: {current_count:6d} -> {len(sampled):6d} (ä¸‹é‡‡æ ·)")
        else:
            sampled = samples
            print(f"  âœ… {label:15s}: {current_count:6d} (æ— éœ€å¹³è¡¡)")
        
        balanced[label] = sampled
    
    return balanced


def create_training_sample(sample_dict: Dict, sample_id: int) -> Dict:
    """
    åˆ›å»ºè®­ç»ƒæ ·æœ¬ï¼ˆæç®€æ ¼å¼ï¼‰
    
    æ ¼å¼:
    Input: /path?x=1
    Output: 1|SQLi
    """
    return {
        'id': f'sample_{sample_id:06d}',
        'input': sample_dict['url'],
        'output': sample_dict['standard_label'],
        'raw_label': sample_dict['label'],
        'source': sample_dict['source']
    }


def save_error_log(errors: List[str], output_dir: str):
    """ä¿å­˜é”™è¯¯æ—¥å¿—"""
    if not errors:
        return
    
    log_path = os.path.join(output_dir, 'error.log')
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(errors))
    print(f"\n  ğŸ“„ é”™è¯¯æ—¥å¿—: {log_path} ({len(errors)} æ¡)")


def main():
    print("=" * 70)
    print("ğŸ”§ åœ¨çº¿æ£€æµ‹æ•°æ®å‡†å¤‡ (LoRA-online)")
    print("=" * 70)
    
    # ========== åŠ è½½æ‰€æœ‰æ•°æ®æº ==========
    print("\nğŸ“‚ åŠ è½½æ•°æ®æº...")
    
    all_samples = []
    load_errors = []
    
    for source_name, config in DATA_SOURCE_CONFIG.items():
        print(f"\n  ğŸ“¦ {source_name}:")
        base_dir = config['base_dir']
        
        for filename, label in config['files'].items():
            filepath = os.path.join(base_dir, filename)
            samples = load_urls_from_file(filepath, label, source_name)
            
            if samples:
                all_samples.extend(samples)
                print(f"    âœ… {filename:30s} â†’ {len(samples):6d} æ¡ ({label})")
            else:
                load_errors.append(f"åŠ è½½å¤±è´¥: {filepath}")
    
    print(f"\nğŸ“Š åŸå§‹æ ·æœ¬æ€»æ•°: {len(all_samples)} æ¡")
    
    # ========== å»é‡ ==========
    print("\nğŸ”„ æ•°æ®å»é‡...")
    all_samples = deduplicate_samples(all_samples)
    print(f"  å»é‡åæ€»æ•°: {len(all_samples)} æ¡")
    
    # ========== æŒ‰æ ‡ç­¾åˆ†ç»„ ==========
    print("\nğŸ“Š æŒ‰æ ‡ç­¾åˆ†ç»„ç»Ÿè®¡...")
    samples_by_label = defaultdict(list)
    for sample in all_samples:
        samples_by_label[sample['label']].append(sample)
    
    for label, samples in sorted(samples_by_label.items()):
        print(f"  {label:15s}: {len(samples):6d} æ¡")
    
    # ========== æ•°æ®å¹³è¡¡ ==========
    print("\nâš–ï¸  æ•°æ®å¹³è¡¡å¤„ç†...")
    balanced_samples = balance_samples(samples_by_label)
    
    # ========== åˆå¹¶å¹¶æ‰“ä¹± ==========
    print("\nğŸ”€ åˆå¹¶å¹¶æ‰“ä¹±æ•°æ®...")
    final_samples = []
    for label, samples in balanced_samples.items():
        final_samples.extend(samples)
    
    random.shuffle(final_samples)
    print(f"  æœ€ç»ˆæ ·æœ¬æ•°: {len(final_samples)} æ¡")
    
    # ========== ç”Ÿæˆè®­ç»ƒæ ·æœ¬ ==========
    print("\nğŸ”„ ç”Ÿæˆè®­ç»ƒæ ·æœ¬...")
    training_data = [
        create_training_sample(sample, i)
        for i, sample in enumerate(final_samples)
    ]
    
    # ========== åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›† ==========
    print("\nâœ‚ï¸  åˆ’åˆ†æ•°æ®é›†...")
    split_ratio = 0.9
    split_idx = int(len(training_data) * split_ratio)
    
    train_data = training_data[:split_idx]
    val_data = training_data[split_idx:]
    
    print(f"  è®­ç»ƒé›†: {len(train_data):6d} æ¡ ({split_ratio*100:.0f}%)")
    print(f"  éªŒè¯é›†: {len(val_data):6d} æ¡ ({(1-split_ratio)*100:.0f}%)")
    
    # ========== ä¿å­˜æ•°æ® ==========
    print("\nğŸ’¾ ä¿å­˜æ•°æ®...")
    output_dir = './data/finetune_online/raw'   #è¿™é‡Œæµ‹è¯•ä½¿ç”¨
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒé›†
    train_path = os.path.join(output_dir, 'train.jsonl')
    with open(train_path, 'w', encoding='utf-8') as f:
        for sample in train_data:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    print(f"  âœ… {train_path}")
    
    # ä¿å­˜éªŒè¯é›†
    val_path = os.path.join(output_dir, 'val.jsonl')
    with open(val_path, 'w', encoding='utf-8') as f:
        for sample in val_data:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    print(f"  âœ… {val_path}")
    
    # ä¿å­˜æ ·æœ¬é¢„è§ˆ
    preview_samples = {
        label: [s for s in train_data if s['raw_label'] == label][:5]
        for label in STANDARD_LABELS.keys()
    }
    preview_path = os.path.join(output_dir, 'sample_preview.json')
    with open(preview_path, 'w', encoding='utf-8') as f:
        json.dump(preview_samples, f, ensure_ascii=False, indent=2)
    print(f"  âœ… {preview_path}")
    
    # ä¿å­˜æ•°æ®ç»Ÿè®¡
    stats = {
        'total_samples': len(final_samples),
        'train_samples': len(train_data),
        'val_samples': len(val_data),
        'distribution': {
            label: len([s for s in training_data if s['raw_label'] == label])
            for label in STANDARD_LABELS.keys()
        },
        'sources': list(DATA_SOURCE_CONFIG.keys())
    }
    stats_path = os.path.join(output_dir, 'data_stats.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"  âœ… {stats_path}")
    
    # ä¿å­˜é”™è¯¯æ—¥å¿—
    save_error_log(load_errors, output_dir)
    
    # ========== å®Œæˆ ==========
    print("\n" + "=" * 70)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
    print("=" * 70)
    
    print("\nğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for label, count in stats['distribution'].items():
        percentage = count / len(training_data) * 100
        print(f"  {label:15s}: {count:6d} æ¡ ({percentage:5.1f}%)")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"  1. æŸ¥çœ‹æ ·æœ¬é¢„è§ˆ: {preview_path}")
    print(f"  2. æ£€æŸ¥æ•°æ®ç»Ÿè®¡: {stats_path}")
    print("  3. è¿è¡Œè®­ç»ƒè„šæœ¬: python scripts/train_lora_online.py")
    print("=" * 70)


if __name__ == "__main__":
    main()