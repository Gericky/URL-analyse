"""
LoRA å¾®è°ƒè„šæœ¬ - URLå¨èƒæ£€æµ‹æŒ‡ä»¤å¾®è°ƒï¼ˆæ”¯æŒæ–­ç‚¹ç»­è®­ï¼‰
"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel
import glob
import json

# ========================
# è·¯å¾„ä¸åŸºç¡€é…ç½®
# ========================

BASE_MODEL = "d:/code/URL-analyse/Qwen3-0.6B"
DATA_DIR = "d:/code/URL-analyse/script/data/finetune_online/raw"
TRAIN_PATH = os.path.join(DATA_DIR, "train.jsonl")
VAL_PATH = os.path.join(DATA_DIR, "val.jsonl")
OUTPUT_DIR = "d:/code/URL-analyse/script/output/lora_online_v2"

# âœ¨âœ¨âœ¨ æ˜¯å¦ä»æœ€æ–° checkpoint æ¢å¤è®­ç»ƒ
RESUME_FROM_CHECKPOINT = True

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ========================
# æ£€æµ‹æœ€æ–° checkpoint
# ========================

def get_latest_checkpoint(output_dir):
    """æŸ¥æ‰¾æœ€æ–°çš„ checkpoint ç›®å½•"""
    checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
    
    if not checkpoints:
        return None, 0
    
    # æŒ‰æ­¥æ•°æ’åº
    checkpoints = sorted(checkpoints, key=lambda x: int(x.split("-")[-1]))
    latest = checkpoints[-1]
    latest_step = int(latest.split("-")[-1])
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(checkpoints)} ä¸ª checkpoint:")
    for ckpt in checkpoints[-5:]:
        step = ckpt.split("-")[-1]
        marker = " â† æœ€æ–°" if ckpt == latest else ""
        print(f"   checkpoint-{step}{marker}")
    
    return latest, latest_step


def get_training_state(checkpoint_path):
    """
    è¯»å– checkpoint çš„è®­ç»ƒçŠ¶æ€
    
    Returns:
        dict: åŒ…å« epoch, global_step, total_steps ç­‰ä¿¡æ¯
    """
    trainer_state_path = os.path.join(checkpoint_path, "trainer_state.json")
    
    if not os.path.exists(trainer_state_path):
        return None
    
    with open(trainer_state_path, 'r') as f:
        state = json.load(f)
    
    return {
        'epoch': state.get('epoch', 0),
        'global_step': state.get('global_step', 0),
        'max_steps': state.get('max_steps', 0),
        'total_flos': state.get('total_flos', 0)
    }


# æŸ¥æ‰¾ checkpoint
resume_checkpoint = None
resume_step = 0

if RESUME_FROM_CHECKPOINT:
    resume_checkpoint, resume_step = get_latest_checkpoint(OUTPUT_DIR)
    
    if resume_checkpoint:
        state = get_training_state(resume_checkpoint)
        if state:
            print(f"\nâœ… å°†ä»ä»¥ä¸‹ checkpoint æ¢å¤è®­ç»ƒ:")
            print(f"   è·¯å¾„: {resume_checkpoint}")
            print(f"   å·²è®­ç»ƒ: {state['global_step']} æ­¥ (Epoch {state['epoch']:.2f})")
            print(f"   æ€»æ­¥æ•°: {state['max_steps']} æ­¥")
            resume_step = state['global_step']
        else:
            print(f"\nâš ï¸  æ— æ³•è¯»å–è®­ç»ƒçŠ¶æ€ï¼Œå°†ä»æ­¥æ•° {resume_step} ä¼°ç®—æ¢å¤")
    else:
        print(f"\nâš ï¸  æœªæ‰¾åˆ° checkpointï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
else:
    print(f"\nğŸ”„ ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆå·²ç¦ç”¨æ–­ç‚¹ç»­è®­ï¼‰")

# ========================
# æ¨¡å‹ä¸ Tokenizer åŠ è½½
# ========================

print("\nğŸš€ åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨...")

tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL, 
    use_fast=False,
    trust_remote_code=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

# é…ç½® 4-bit é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# âœ¨âœ¨âœ¨ æ ¹æ®æ˜¯å¦æœ‰ checkpoint å†³å®šåŠ è½½æ–¹å¼
if resume_checkpoint:
    print(f"ğŸ“‚ ä» checkpoint åŠ è½½æ¨¡å‹: {os.path.basename(resume_checkpoint)}")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
    
    model = prepare_model_for_kbit_training(model)
    
    # åŠ è½½ LoRA adapter
    model = PeftModel.from_pretrained(
        model,
        resume_checkpoint,
        is_trainable=True  # âš ï¸ å¿…é¡»è®¾ä¸º True
    )
    
    print(f"âœ… å·²åŠ è½½ LoRA æƒé‡ (æ­¥æ•°: {resume_step})")
    
else:
    print(f"ğŸ“‚ ä»é›¶å¼€å§‹åˆå§‹åŒ–æ¨¡å‹")
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
    
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        # target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)

model.print_trainable_parameters()

# ========================
# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ========================

print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")

dataset = load_dataset(
    "json",
    data_files={"train": TRAIN_PATH, "validation": VAL_PATH}
)

print(f"âœ… è®­ç»ƒé›†: {len(dataset['train'])} æ¡")
print(f"âœ… éªŒè¯é›†: {len(dataset['validation'])} æ¡")

# ========================
# æŒ‡ä»¤æ ¼å¼åŒ–
# ========================

PROMPT_TEMPLATE = """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªURLå®‰å…¨æ£€æµ‹ç³»ç»Ÿ,ä¸“é—¨è¯†åˆ«æ¶æ„URLã€‚è¯·åˆ¤æ–­ç»™å®šURLæ˜¯å¦å­˜åœ¨å¨èƒã€‚
è¾“å‡ºæ ¼å¼è¦æ±‚: 
- å¦‚æœURLå®‰å…¨,è¾“å‡º: 0|benign
- å¦‚æœURLå­˜åœ¨å¨èƒ,è¾“å‡º: 1|å¨èƒç±»å‹ (å¦‚ SQli,XSS,RCEç­‰)
<|im_end|>
<|im_start|>user
{instruction}
è¾“å…¥URL: {input}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>"""

def format_instruction(example):
    """å°†æ•°æ®æ ¼å¼åŒ–ä¸ºæŒ‡ä»¤æ¨¡æ¿"""
    # âœ¨ å¤„ç†å¯èƒ½ç¼ºå¤± instruction å­—æ®µçš„æƒ…å†µ
    instruction = example.get("instruction", "åˆ¤æ–­ä»¥ä¸‹URLæ˜¯å¦å­˜åœ¨å®‰å…¨å¨èƒ")
    
    prompt = PROMPT_TEMPLATE.format(
        instruction=instruction,
        input=example["input"],
        output=example["output"]
    )
    return {"text": prompt}

print("\nğŸ“ æ ¼å¼åŒ–æŒ‡ä»¤æ•°æ®...")
dataset = dataset.map(format_instruction, remove_columns=dataset["train"].column_names)

# ========================
# Tokenize
# ========================

def tokenize_fn(examples):
    """æ‰¹é‡åˆ†è¯å¤„ç†"""
    model_inputs = {"input_ids": [], "attention_mask": [], "labels": []}
    
    for text in examples["text"]:
        tokenized = tokenizer(
            text,
            truncation=True,
            max_length=512,
            padding=False,
            add_special_tokens=True
        )
        
        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
        
        # æ‰¾åˆ° assistant å¼€å§‹ä½ç½®ï¼Œåªå¯¹è¾“å‡ºéƒ¨åˆ†è®¡ç®—æŸå¤±
        assistant_marker = "<|im_start|>assistant\n"
        assistant_start_idx = text.find(assistant_marker)
        
        if assistant_start_idx != -1:
            prefix = text[:assistant_start_idx + len(assistant_marker)]
            prefix_ids = tokenizer(
                prefix, 
                add_special_tokens=True,
                truncation=True,
                max_length=512
            )["input_ids"]
            
            prefix_len = len(prefix_ids)
            
            if prefix_len > len(input_ids):
                prefix_len = len(input_ids)
            
            labels = [-100] * prefix_len + input_ids[prefix_len:]
            
            if len(labels) > len(input_ids):
                labels = labels[:len(input_ids)]
            elif len(labels) < len(input_ids):
                labels = labels + [-100] * (len(input_ids) - len(labels))
        else:
            labels = input_ids.copy()
        
        model_inputs["input_ids"].append(input_ids)
        model_inputs["attention_mask"].append(attention_mask)
        model_inputs["labels"].append(labels)
    
    return model_inputs

print("\nâœ‚ï¸ è¿›è¡Œåˆ†è¯...")
tokenized_datasets = dataset.map(
    tokenize_fn,
    batched=True,
    remove_columns=["text"],
    desc="Tokenizing"
)

print(f"âœ… Tokenize å®Œæˆ")

# ========================
# æ•°æ®æ•´ç†å™¨
# ========================

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    label_pad_token_id=-100,
    pad_to_multiple_of=8
)

# ========================
# è®­ç»ƒå‚æ•°
# ========================

# âœ¨âœ¨âœ¨ è®¡ç®—å‰©ä½™è®­ç»ƒæ­¥æ•°
total_epochs = 5
per_device_batch_size = 2
gradient_accumulation = 8
effective_batch_size = per_device_batch_size * gradient_accumulation

# è®¡ç®—æ€»æ­¥æ•°
num_samples = len(tokenized_datasets["train"])
steps_per_epoch = num_samples // effective_batch_size
total_steps = steps_per_epoch * total_epochs

# å¦‚æœä» checkpoint æ¢å¤ï¼Œè®¡ç®—å‰©ä½™æ­¥æ•°
if resume_checkpoint and resume_step > 0:
    remaining_steps = total_steps - resume_step
    remaining_epochs = remaining_steps / steps_per_epoch
    
    print(f"\nğŸ“Š è®­ç»ƒè¿›åº¦:")
    print(f"   æ€»æ­¥æ•°: {total_steps}")
    print(f"   å·²å®Œæˆ: {resume_step} æ­¥")
    print(f"   å‰©ä½™: {remaining_steps} æ­¥ ({remaining_epochs:.2f} epochs)")
else:
    remaining_steps = total_steps
    print(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   æ€»æ­¥æ•°: {total_steps}")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=total_epochs,
    per_device_train_batch_size=per_device_batch_size,
    gradient_accumulation_steps=gradient_accumulation,
    learning_rate=3e-4,
    warmup_ratio=0.1,
    logging_steps=10,
    save_steps=100,
    eval_strategy="steps",
    eval_steps=100,
    save_total_limit=3,
    bf16=True,
    lr_scheduler_type="cosine",
    optim="paged_adamw_8bit",
    report_to="none",
    gradient_checkpointing=True,
    dataloader_num_workers=0,
    remove_unused_columns=False,
    max_grad_norm=0.3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    # âœ¨âœ¨âœ¨ å…³é”®å‚æ•°
    ignore_data_skip=False,  # âš ï¸ è®¾ä¸º Falseï¼Œè®© Trainer è·³è¿‡å·²è®­ç»ƒçš„æ•°æ®
)

# ========================
# Trainer è®¾ç½®
# ========================

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer
)

# ========================
# å¼€å§‹è®­ç»ƒ
# ========================

print("\n" + "=" * 70)
if resume_checkpoint:
    print("ğŸ”„ ä» checkpoint æ¢å¤è®­ç»ƒ...")
    print(f"ğŸ“ èµ·å§‹æ­¥æ•°: {resume_step}")
    print(f"ğŸ“ å‰©ä½™æ­¥æ•°: {remaining_steps}")
else:
    print("ğŸš€ å¼€å§‹å…¨æ–° LoRA å¾®è°ƒ...")

print("=" * 70)
print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
print(f"   - è®­ç»ƒæ ·æœ¬: {len(tokenized_datasets['train'])}")
print(f"   - éªŒè¯æ ·æœ¬: {len(tokenized_datasets['validation'])}")
print(f"   - Batch Size: {per_device_batch_size}")
print(f"   - æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation}")
print(f"   - æœ‰æ•ˆ Batch: {effective_batch_size}")
print(f"   - å­¦ä¹ ç‡: {training_args.learning_rate}")
print(f"   - æ€» Epochs: {total_epochs}")
print(f"   - æ¯ Epoch æ­¥æ•°: {steps_per_epoch}")
print("=" * 70)

# âœ¨âœ¨âœ¨ å…³é”®ï¼šä¼ å…¥ resume_from_checkpoint å‚æ•°
trainer.train(resume_from_checkpoint=resume_checkpoint)

# ========================
# ä¿å­˜æ¨¡å‹
# ========================

print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("\n" + "=" * 70)
print("âœ… LoRA å¾®è°ƒå®Œæˆ!")
print("=" * 70)
print(f"ğŸ“ æ¨¡å‹è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
print(f"ğŸ“Š æœ€ç»ˆæ­¥æ•°: {trainer.state.global_step}")
print("=" * 70)