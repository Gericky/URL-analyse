"""
LoRA å¾®è°ƒè„šæœ¬ - URLå¨èƒæ£€æµ‹æŒ‡ä»¤å¾®è°ƒ
è¾“å…¥: URL
è¾“å‡º: 0|benign æˆ– 1|threat_type
"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ========================
# è·¯å¾„ä¸åŸºç¡€é…ç½®
# ========================

BASE_MODEL = "d:/code/URL-analyse/Qwen3-0.6B"
DATA_DIR = "d:/code/URL-analyse/script/data/finetune_online"
# DATA_DIR = "d:/code/URL-analyse/script/data/finetune_online/test"
TRAIN_PATH = os.path.join(DATA_DIR, "train.jsonl")
VAL_PATH = os.path.join(DATA_DIR, "val.jsonl")
OUTPUT_DIR = "d:/code/URL-analyse/script/output/lora_online"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ========================
# æ¨¡å‹ä¸ Tokenizer åŠ è½½
# ========================

print("ğŸš€ åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨...")

tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL, 
    use_fast=False,
    trust_remote_code=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

# é…ç½® 4-bit é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# ========================
# LoRA é…ç½®
# ========================

print("âš™ï¸ è®¾ç½® LoRA å‚æ•°...")
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ========================
# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ========================

print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")

if not os.path.exists(TRAIN_PATH):
    raise FileNotFoundError(f"è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {TRAIN_PATH}")
if not os.path.exists(VAL_PATH):
    raise FileNotFoundError(f"éªŒè¯æ•°æ®ä¸å­˜åœ¨: {VAL_PATH}")

dataset = load_dataset(
    "json",
    data_files={"train": TRAIN_PATH, "validation": VAL_PATH}
)

print(f"âœ… è®­ç»ƒé›†æ ·æœ¬æ•°: {len(dataset['train'])}")
print(f"âœ… éªŒè¯é›†æ ·æœ¬æ•°: {len(dataset['validation'])}")

# æ‰“å°æ•°æ®ç¤ºä¾‹
print("\nğŸ“ æ•°æ®é›†ç¤ºä¾‹:")
sample = dataset["train"][0]
print(f"  instruction: {sample['instruction']}")
print(f"  input: {sample['input'][:80]}...")
print(f"  output: {sample['output']}")

# ========================
# æŒ‡ä»¤æ ¼å¼åŒ–
# ========================

PROMPT_TEMPLATE = """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªURLå®‰å…¨æ£€æµ‹ç³»ç»Ÿ,ä¸“é—¨è¯†åˆ«æ¶æ„URLã€‚è¯·åˆ¤æ–­ç»™å®šURLæ˜¯å¦å­˜åœ¨å¨èƒã€‚
è¾“å‡ºæ ¼å¼è¦æ±‚: 
- å¦‚æœURLå®‰å…¨,è¾“å‡º: 0|benign
- å¦‚æœURLå­˜åœ¨å¨èƒ,è¾“å‡º: 1|å¨èƒç±»å‹ (å¦‚ phishing, malware, defacement ç­‰)
<|im_end|>
<|im_start|>user
{instruction}
è¾“å…¥URL: {input}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>"""

def format_instruction(example):
    """å°†æ•°æ®æ ¼å¼åŒ–ä¸ºæŒ‡ä»¤æ¨¡æ¿"""
    prompt = PROMPT_TEMPLATE.format(
        instruction=example["instruction"],
        input=example["input"],
        output=example["output"]
    )
    return {"text": prompt}

print("\nğŸ“ æ ¼å¼åŒ–æŒ‡ä»¤æ•°æ®...")
dataset = dataset.map(format_instruction, remove_columns=dataset["train"].column_names)

# æ‰“å°æ ¼å¼åŒ–åçš„ç¤ºä¾‹
print("\nğŸ“„ æ ¼å¼åŒ–åçš„å®Œæ•´æç¤ºè¯ç¤ºä¾‹:")
print(dataset["train"][0]["text"][:500] + "...")

# ========================
# Tokenize
# ========================

def tokenize_fn(examples):
    """æ‰¹é‡åˆ†è¯å¤„ç† - åªå¯¹ assistant çš„è¾“å‡ºè®¡ç®—æŸå¤±"""
    
    model_inputs = {"input_ids": [], "attention_mask": [], "labels": []}
    
    for text in examples["text"]:
        # Tokenize å®Œæ•´æ–‡æœ¬
        tokenized = tokenizer(
            text,
            truncation=True,
            max_length=512,
            padding=False,
            add_special_tokens=True
        )
        
        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
        
        # æ‰¾åˆ° assistant å¼€å§‹ä½ç½®
        assistant_marker = "<|im_start|>assistant\n"
        assistant_start_idx = text.find(assistant_marker)
        
        if assistant_start_idx != -1:
            # åˆ†åˆ« tokenize prefix å’Œå®Œæ•´æ–‡æœ¬
            prefix = text[:assistant_start_idx + len(assistant_marker)]
            
            # Tokenize prefix (ä¸æ·»åŠ ç‰¹æ®Š token,å› ä¸ºå®Œæ•´æ–‡æœ¬å·²ç»æ·»åŠ äº†)
            prefix_ids = tokenizer(
                prefix, 
                add_special_tokens=True,
                truncation=True,
                max_length=512
            )["input_ids"]
            
            prefix_len = len(prefix_ids)
            
            # ç¡®ä¿ prefix_len ä¸è¶…è¿‡ input_ids é•¿åº¦
            if prefix_len > len(input_ids):
                prefix_len = len(input_ids)
            
            # åˆ›å»º labels: assistant ä¹‹å‰çš„éƒ¨åˆ†è®¾ä¸º -100
            labels = [-100] * prefix_len + input_ids[prefix_len:]
            
            # ç¡®ä¿ labels å’Œ input_ids é•¿åº¦ä¸€è‡´
            if len(labels) > len(input_ids):
                labels = labels[:len(input_ids)]
            elif len(labels) < len(input_ids):
                labels = labels + [-100] * (len(input_ids) - len(labels))
        else:
            # å¦‚æœæ²¡æ‰¾åˆ° assistant æ ‡è®°,æ•´ä¸ªåºåˆ—éƒ½è®¡ç®—æŸå¤±
            labels = input_ids.copy()
        
        model_inputs["input_ids"].append(input_ids)
        model_inputs["attention_mask"].append(attention_mask)
        model_inputs["labels"].append(labels)
    
    return model_inputs

print("\nâœ‚ï¸ è¿›è¡Œåˆ†è¯...")
tokenized_datasets = dataset.map(
    tokenize_fn,
    batched=True,
    remove_columns=["text"],
    desc="Tokenizing"
)

print(f"âœ… Tokenize å®Œæˆ")
# éªŒè¯æ•°æ®é•¿åº¦
sample = tokenized_datasets['train'][0]
print(f"   è®­ç»ƒé›†æ ·æœ¬ç¤ºä¾‹:")
print(f"   - input_ids é•¿åº¦: {len(sample['input_ids'])}")
print(f"   - labels é•¿åº¦: {len(sample['labels'])}")
print(f"   - attention_mask é•¿åº¦: {len(sample['attention_mask'])}")

# æ£€æŸ¥æ‰€æœ‰æ ·æœ¬çš„é•¿åº¦ä¸€è‡´æ€§
print("\nğŸ” éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
for i in range(min(5, len(tokenized_datasets['train']))):
    sample = tokenized_datasets['train'][i]
    input_len = len(sample['input_ids'])
    label_len = len(sample['labels'])
    mask_len = len(sample['attention_mask'])
    
    if input_len != label_len or input_len != mask_len:
        print(f"âš ï¸ æ ·æœ¬ {i} é•¿åº¦ä¸ä¸€è‡´: input_ids={input_len}, labels={label_len}, attention_mask={mask_len}")
    else:
        print(f"âœ… æ ·æœ¬ {i} é•¿åº¦ä¸€è‡´: {input_len}")

# ========================
# æ•°æ®æ•´ç†å™¨
# ========================

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    label_pad_token_id=-100,
    pad_to_multiple_of=8
)

# ========================
# è®­ç»ƒå‚æ•°
# ========================

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=3e-4,
    warmup_ratio=0.1,
    logging_steps=10,
    save_steps=100,
    eval_strategy="steps",
    eval_steps=100,
    save_total_limit=3,
    bf16=True,
    lr_scheduler_type="cosine",
    optim="paged_adamw_8bit",
    report_to="none",
    gradient_checkpointing=True,
    dataloader_num_workers=0,
    remove_unused_columns=False,
    max_grad_norm=0.3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss"
)

# ========================
# Trainer è®¾ç½®
# ========================

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer
)

# ========================
# å¼€å§‹è®­ç»ƒ
# ========================

print("\n" + "=" * 70)
print("ğŸš€ å¼€å§‹ LoRA å¾®è°ƒ...")
print("=" * 70)
print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
print(f"   - è®­ç»ƒæ ·æœ¬æ•°: {len(tokenized_datasets['train'])}")
print(f"   - éªŒè¯æ ·æœ¬æ•°: {len(tokenized_datasets['validation'])}")
print(f"   - Batch Size: {training_args.per_device_train_batch_size}")
print(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {training_args.gradient_accumulation_steps}")
print(f"   - æœ‰æ•ˆ Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"   - å­¦ä¹ ç‡: {training_args.learning_rate}")
print(f"   - è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
print(f"   - æœ€å¤§åºåˆ—é•¿åº¦: 512")
print("=" * 70)

trainer.train()

# ========================
# ä¿å­˜æ¨¡å‹
# ========================

print("\nğŸ’¾ ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("\n" + "=" * 70)
print("âœ… LoRA å¾®è°ƒå®Œæˆ!")
print("=" * 70)
print(f"ğŸ“ æ¨¡å‹è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
print("\nğŸ“Œ ä½¿ç”¨ç¤ºä¾‹:")
print("```python")
print("from peft import PeftModel")
print("from transformers import AutoModelForCausalLM, AutoTokenizer")
print("")
print(f"# åŠ è½½æ¨¡å‹")
print(f"base_model = AutoModelForCausalLM.from_pretrained('{BASE_MODEL}')")
print(f"model = PeftModel.from_pretrained(base_model, '{OUTPUT_DIR}')")
print(f"tokenizer = AutoTokenizer.from_pretrained('{OUTPUT_DIR}')")
print("model.eval()")
print("")
print("# æ¨ç†ç¤ºä¾‹")
print('prompt = """<|im_start|>system')
print("ä½ æ˜¯ä¸€ä¸ªURLå®‰å…¨æ£€æµ‹ç³»ç»Ÿ,ä¸“é—¨è¯†åˆ«æ¶æ„URLã€‚è¯·åˆ¤æ–­ç»™å®šURLæ˜¯å¦å­˜åœ¨å¨èƒã€‚")
print("è¾“å‡ºæ ¼å¼è¦æ±‚: ")
print("- å¦‚æœURLå®‰å…¨,è¾“å‡º: 0|benign")
print("- å¦‚æœURLå­˜åœ¨å¨èƒ,è¾“å‡º: 1|å¨èƒç±»å‹")
print('<|im_end|>')
print('<|im_start|>user')
print("åˆ¤æ–­ä»¥ä¸‹URLæ˜¯å¦å­˜åœ¨å®‰å…¨å¨èƒ")
print('è¾“å…¥URL: http://suspicious-phishing-site.com/login<|im_end|>')
print('<|im_start|>assistant')
print('"""')
print("")
print("inputs = tokenizer(prompt, return_tensors='pt').to(model.device)")
print("outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)")
print("result = tokenizer.decode(outputs[0], skip_special_tokens=False)")
print("")
print("# æå–é¢„æµ‹ç»“æœ")
print("prediction = result.split('<|im_start|>assistant')[-1].split('<|im_end|>')[0].strip()")
print("print(f'é¢„æµ‹ç»“æœ: {prediction}')")
print("```")
print("=" * 70)