import csv
import os

def detect_encoding(file_path):
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    encodings = ['utf-8-sig', 'utf-8', 'gbk', 'gb2312', 'latin1', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                f.read()
            return encoding
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    return 'utf-8'

def clean_line(line):
    """æ¸…ç†è¡Œå†…å®¹ï¼Œç§»é™¤NULå­—ç¬¦å’Œå…¶ä»–ä¸å¯è§å­—ç¬¦"""
    if line is None:
        return ""
    return ''.join(char for char in line if char.isprintable() or char in '\n\r\t')

def process_csv_files():
    # å®šä¹‰è·¯å¾„
    raw_dir = 'raw/WAF-github'
    total_dir = 'processed/WAF-github/total'
    
    # ç¡®ä¿totalç›®å½•å­˜åœ¨
    os.makedirs(total_dir, exist_ok=True)
    
    # æ ¹æ®æ–‡ä»¶ååˆ†ç±»å­˜å‚¨
    url_categories = {
        'sqli': [],           # SQLæ³¨å…¥
        'xss': [],            # XSSæ”»å‡»
        'normal': []          # æ­£å¸¸URL
    }
    
    # è·å–rawç›®å½•ä¸‹æ‰€æœ‰csvæ–‡ä»¶
    csv_files = [f for f in os.listdir(raw_dir) if f.endswith('.csv')]
    
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    # å¤„ç†æ¯ä¸ªCSVæ–‡ä»¶
    for csv_file in csv_files:
        file_path = os.path.join(raw_dir, csv_file)
        print(f"\nå¤„ç†æ–‡ä»¶: {csv_file}")
        
        # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ”»å‡»ç±»å‹
        if 'sqli' in csv_file.lower() or 'sql' in csv_file.lower():
            attack_category = 'sqli'
        elif 'xss' in csv_file.lower():
            attack_category = 'xss'
        else:
            attack_category = 'unknown'
        
        # æ£€æµ‹ç¼–ç 
        encoding = detect_encoding(file_path)
        print(f"  æ£€æµ‹åˆ°ç¼–ç : {encoding}")
        print(f"  æ”»å‡»ç±»å‹: {attack_category}")
        
        try:
            # è¯»å–å¹¶æ¸…ç†æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                raw_content = f.read()
            
            # ç§»é™¤NULå­—ç¬¦å’Œä¸å¯è§å­—ç¬¦
            cleaned_content = clean_line(raw_content)
            
            # ä½¿ç”¨æ¸…ç†åçš„å†…å®¹åˆ›å»ºä¸´æ—¶CSVè¯»å–å™¨
            from io import StringIO
            reader = csv.DictReader(StringIO(cleaned_content))
            
            count = 0
            skipped = 0
            for row in reader:
                try:
                    # è·å–URLå’Œæ ‡ç­¾ï¼Œå¹¶æ¸…ç†
                    url = clean_line(row.get('Sentence', '')).strip()
                    label = clean_line(row.get('Label', '')).strip()
                    
                    # è·³è¿‡ç©ºè¡Œ
                    if not url or not label:
                        skipped += 1
                        continue
                    
                    # æ ¹æ®æ ‡ç­¾åˆ†ç±»
                    if label == '0':
                        # æ­£å¸¸URL
                        url_categories['normal'].append(url)
                    else:
                        # æ”»å‡»URL - æ ¹æ®æ–‡ä»¶ååˆ†ç±»
                        if attack_category in url_categories:
                            url_categories[attack_category].append(url)
                        else:
                            # æœªçŸ¥ç±»å‹ä¹Ÿè®°å½•
                            if 'unknown' not in url_categories:
                                url_categories['unknown'] = []
                            url_categories['unknown'].append(url)
                    
                    count += 1
                    
                except Exception as row_error:
                    skipped += 1
                    continue
            
            print(f"  âœ… æˆåŠŸå¤„ç† {count} æ¡è®°å½•" + (f" (è·³è¿‡ {skipped} æ¡)" if skipped > 0 else ""))
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {str(e)}")
            continue
    
    # ä¿å­˜åˆ†ç±»ç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ’¾ ä¿å­˜åˆ†ç±»ç»“æœ:")
    print(f"{'='*60}")
    
    for category, urls in url_categories.items():
        if len(urls) > 0:
            # æ–‡ä»¶åæ˜ å°„
            filename_map = {
                'normal': 'normal_urls.txt',
                'sqli': 'sqli_urls.txt',
                'xss': 'xss_urls.txt',
                'unknown': 'unknown_attack_urls.txt'
            }
            
            output_file = os.path.join(total_dir, filename_map.get(category, f'{category}_urls.txt'))
            
            with open(output_file, 'w', encoding='utf-8') as f:
                for url in urls:
                    f.write(url + '\n')
            
            print(f"  ğŸ“„ {category.upper():10s}: {len(urls):5d} æ¡ -> {output_file}")
    
    print(f"{'='*60}")
    print(f"\nå¤„ç†å®Œæˆ!")
    print(f"{'='*60}")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"   æ­£å¸¸URL:   {len(url_categories['normal']):5d} æ¡")
    print(f"   SQLæ³¨å…¥:   {len(url_categories['sqli']):5d} æ¡")
    print(f"   XSSæ”»å‡»:   {len(url_categories['xss']):5d} æ¡")
    if 'unknown' in url_categories:
        print(f"   æœªçŸ¥ç±»å‹:  {len(url_categories['unknown']):5d} æ¡")
    print(f"   æ€»è®¡:      {sum(len(urls) for urls in url_categories.values()):5d} æ¡")
    print(f"{'='*60}")

if __name__ == '__main__':
    process_csv_files()