import os
import json
from glob import glob
from collections import defaultdict

def process_atrdf():
    """å¤„ç†ATRDFæ•°æ®é›†ï¼ŒæŒ‰æ”»å‡»ç±»å‹æå–URL"""
    
    # å®šä¹‰è·¯å¾„
    raw_dir = 'raw/ATRDF-github/train'
    total_dir = 'processed/ATRDF/total'
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(total_dir, exist_ok=True)
    
    # å­˜å‚¨æŒ‰æ”»å‡»ç±»å‹åˆ†ç±»çš„URL (ä½¿ç”¨setè‡ªåŠ¨å»é‡)
    attack_urls = defaultdict(set)
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = glob(os.path.join(raw_dir, '*.json'))
    
    if not json_files:
        print(f"âŒ æœªæ‰¾åˆ°JSONæ–‡ä»¶: {raw_dir}")
        return
    
    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶\n")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    total_records = 0
    skipped_records = 0
    skip_reasons = defaultdict(int)  # ç»Ÿè®¡è·³è¿‡åŸå› 
    
    for json_file in json_files:
        print(f"å¤„ç†æ–‡ä»¶: {os.path.basename(json_file)}")
        
        try:
            # å°è¯•å¤šç§ç¼–ç 
            content = None
            for encoding in ['utf-8', 'utf-8-sig', 'latin1', 'gbk']:
                try:
                    with open(json_file, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if not content:
                print(f"  âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶")
                continue
            
            # å°è¯•è§£æJSON
            data = []
            try:
                # å…ˆå°è¯•ä½œä¸ºJSONæ•°ç»„
                parsed = json.loads(content)
                if isinstance(parsed, dict):
                    data = [parsed]  # å•ä¸ªå¯¹è±¡è½¬ä¸ºåˆ—è¡¨
                elif isinstance(parsed, list):
                    data = parsed
            except json.JSONDecodeError:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•æŒ‰è¡Œè§£æ(JSONLæ ¼å¼)
                for line_num, line in enumerate(content.strip().split('\n'), 1):
                    line = line.strip().rstrip(',')
                    if line:
                        try:
                            data.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            skip_reasons[f'JSONè§£æå¤±è´¥(è¡Œ{line_num})'] += 1
                            continue
            
            print(f"  ğŸ“¦ è§£æåˆ° {len(data)} æ¡è®°å½•")
            
            # æå–URLå’Œæ”»å‡»ç±»å‹
            file_count = 0
            file_skip = 0
            
            for idx, record in enumerate(data, 1):
                try:
                    # æå–URL
                    url = None
                    if 'request' in record:
                        url = record['request'].get('url')
                    elif 'url' in record:
                        url = record['url']
                    
                    # æå–æ”»å‡»ç±»å‹
                    attack_tag = None
                    if 'request' in record:
                        attack_tag = record['request'].get('Attack_Tag')
                    elif 'Attack_Tag' in record:
                        attack_tag = record['Attack_Tag']
                    
                    # è°ƒè¯•è¾“å‡ºï¼ˆå‰å‡ æ¡ï¼‰
                    if idx <= 3:
                        print(f"    è®°å½•{idx}: URL={'æœ‰' if url else 'æ— '}, Attack_Tag={attack_tag or 'æ— '}")
                    
                    # éªŒè¯å’Œæ¸…ç†
                    if not url:
                        skip_reasons['ç¼ºå°‘URL'] += 1
                        file_skip += 1
                        continue
                    
                    if not attack_tag:
                        skip_reasons['ç¼ºå°‘Attack_Tag'] += 1
                        file_skip += 1
                        continue
                    
                    # å»é™¤æœ«å°¾çš„ HTTP/1.1
                    url = url.strip()
                    if url.endswith(' HTTP/1.1'):
                        url = url[:-9].strip()
                    
                    # æ ‡å‡†åŒ–æ”»å‡»ç±»å‹åç§°
                    attack_tag = attack_tag.strip().upper().replace(' ', '_')
                    
                    # æ·»åŠ åˆ°å¯¹åº”åˆ†ç±»
                    attack_urls[attack_tag].add(url)
                    file_count += 1
                    total_records += 1
                        
                except Exception as e:
                    skip_reasons[f'å¤„ç†å¼‚å¸¸: {type(e).__name__}'] += 1
                    file_skip += 1
                    continue
            
            print(f"  âœ… æˆåŠŸæå– {file_count} æ¡è®°å½• (è·³è¿‡ {file_skip} æ¡)")
            skipped_records += file_skip
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {str(e)}")
            continue
    
    # ä¿å­˜åˆ†ç±»ç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ’¾ ä¿å­˜åˆ†ç±»ç»“æœ:")
    print(f"{'='*60}")
    
    for attack_type, urls in sorted(attack_urls.items()):
        if urls:
            output_file = os.path.join(total_dir, f'{attack_type}.txt')
            
            with open(output_file, 'w', encoding='utf-8') as f:
                for url in sorted(urls):
                    f.write(url + '\n')
            
            print(f"  ğŸ“„ {attack_type:20s}: {len(urls):5d} æ¡ -> {attack_type}.txt")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"{'='*60}")
    print(f"\nå¤„ç†å®Œæˆ!")
    print(f"{'='*60}")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"   æˆåŠŸè®°å½•:  {total_records:5d} æ¡")
    print(f"   è·³è¿‡è®°å½•:  {skipped_records:5d} æ¡")
    print(f"   æ”»å‡»ç±»å‹:  {len(attack_urls):5d} ç§")
    print(f"   æ€»URLæ•°:   {sum(len(urls) for urls in attack_urls.values()):5d} æ¡ (å»é‡å)")
    
    # æ˜¾ç¤ºè·³è¿‡åŸå› 
    if skip_reasons:
        print(f"\nâš ï¸  è·³è¿‡åŸå› ç»Ÿè®¡:")
        for reason, count in sorted(skip_reasons.items(), key=lambda x: x[1], reverse=True):
            print(f"   {reason:30s}: {count:5d} æ¡")
    
    print(f"\næ–‡ä»¶å·²ä¿å­˜åˆ°: {total_dir}")
    print(f"{'='*60}")

if __name__ == '__main__':
    process_atrdf()