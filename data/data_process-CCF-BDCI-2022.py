import os
import csv
import sys

# å¢åŠ CSVå­—æ®µå¤§å°é™åˆ¶ï¼ˆä¿®å¤åçš„ç‰ˆæœ¬ï¼‰
try:
    csv.field_size_limit(sys.maxsize)
except OverflowError:
    # åœ¨æŸäº›ç³»ç»Ÿä¸Š sys.maxsize å¤ªå¤§ï¼Œä½¿ç”¨ä¸€ä¸ªè¾ƒå¤§ä½†å®‰å…¨çš„å€¼
    maxInt = int(1e9)  # 10äº¿å­—ç¬¦ï¼Œè¶³å¤Ÿå¤§
    csv.field_size_limit(maxInt)


def process_csv_file(input_path, output_path, label):
    """
    å¤„ç†CSVæ ¼å¼çš„æ•°æ®æ–‡ä»¶ï¼Œæå–URLåˆ—

    Args:
        input_path: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºTXTæ–‡ä»¶è·¯å¾„
        label: æ ‡ç­¾ç±»å‹ (ç”¨äºè¾“å‡ºæ–‡ä»¶å)
    """
    urls = []

    try:
        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:
            reader = csv.reader(f)

            # è¯»å–è¡¨å¤´
            header = next(reader)
            print(f"ğŸ“‹ è¡¨å¤´: {header}")

            # ç¡®è®¤URLåˆ—ç´¢å¼•
            try:
                url_index = header.index('url')
            except ValueError:
                print(f"âŒ æœªæ‰¾åˆ°'url'åˆ—ï¼Œä½¿ç”¨ç¬¬4åˆ—ï¼ˆç´¢å¼•3ï¼‰")
                url_index = 3

            # è¯»å–æ•°æ®è¡Œ
            for row_num, row in enumerate(reader, start=2):
                if len(row) <= url_index:
                    continue

                url = row[url_index].strip()
                if url:
                    urls.append(url)

        # å»é‡
        urls_unique = list(set(urls))

        # ä¿å­˜åˆ°txt
        with open(output_path, 'w', encoding='utf-8') as f:
            for url in urls_unique:
                f.write(url + '\n')

        print(f"âœ… å·²å¤„ç† {label}: åŸå§‹{len(urls)}æ¡ â†’ å»é‡å{len(urls_unique)}æ¡ â†’ {output_path}")
        return len(urls_unique)

    except Exception as e:
        print(f"âŒ å¤„ç† {input_path} å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0


def main():
    """ä¸»å‡½æ•°:å¤„ç†CCF-BDCI-2022æ•°æ®é›†"""

    # å®šä¹‰è·¯å¾„
    raw_dir = os.path.join(".", "raw", "CCF-BDCI2022-github")
    train_dir = os.path.join(raw_dir, "train")
    test_dir = os.path.join(raw_dir, "test")
    output_dir = os.path.join(".", "processed", "CCF-BDCI2022")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    print(f"\n{'=' * 60}")
    print(f"ğŸ“‚ CCF-BDCI-2022 æ•°æ®é›†å¤„ç†")
    print(f"{'=' * 60}")
    print(f"ğŸ“ è®­ç»ƒé›†ç›®å½•: {os.path.abspath(train_dir)}")
    print(f"ğŸ“ æµ‹è¯•é›†ç›®å½•: {os.path.abspath(test_dir)}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
    print(f"{'=' * 60}\n")

    # å¤„ç†è®­ç»ƒé›†ä¸­çš„å„ç±»æ–‡ä»¶
    train_files = {
        'ç™½.csv': 'normal',
        'SQLæ³¨å…¥.csv': 'sql_injection',
        'XSSè·¨ç«™è„šæœ¬.csv': 'xss',
        'å‘½ä»¤æ‰§è¡Œ.csv': 'command_execution',
        'ç›®å½•éå†.csv': 'path_traversal',
        'è¿œç¨‹ä»£ç æ‰§è¡Œ.csv': 'remote_code_execution'
    }

    # ç»Ÿè®¡
    all_attack_urls = []
    all_normal_urls = []

    print(f"ğŸ“¦ å¤„ç†è®­ç»ƒé›†æ–‡ä»¶:")
    print(f"{'=' * 60}\n")

    for filename, label in train_files.items():
        input_path = os.path.join(train_dir, filename)

        print(f"ğŸ” æ£€æŸ¥æ–‡ä»¶: {os.path.abspath(input_path)}")

        if not os.path.exists(input_path):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†\n")
            continue

        # å•ç‹¬è¾“å‡ºæ–‡ä»¶
        output_path = os.path.join(output_dir, f"{label}.txt")
        count = process_csv_file(input_path, output_path, label)

        # è¯»å–URLæ·»åŠ åˆ°åˆå¹¶åˆ—è¡¨
        if count > 0:
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip()]

                    if label == 'normal':
                        all_normal_urls.extend(urls)
                    else:
                        all_attack_urls.extend(urls)
            except:
                pass
        print()

    # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
    print(f"{'=' * 60}")
    print(f"ğŸ“Š åˆå¹¶å¤„ç†:")
    print(f"{'=' * 60}\n")

    if all_normal_urls:
        normal_output = os.path.join(output_dir, "all_normal.txt")
        normal_unique = list(set(all_normal_urls))
        with open(normal_output, 'w', encoding='utf-8') as f:
            for url in normal_unique:
                f.write(url + '\n')
        print(f"âœ… æ‰€æœ‰æ­£å¸¸URLå·²åˆå¹¶: {len(all_normal_urls)}æ¡ â†’ å»é‡å{len(normal_unique)}æ¡")
        print(f"   â†’ {normal_output}\n")

    if all_attack_urls:
        attack_output = os.path.join(output_dir, "all_attacks.txt")
        attack_unique = list(set(all_attack_urls))
        with open(attack_output, 'w', encoding='utf-8') as f:
            for url in attack_unique:
                f.write(url + '\n')
        print(f"âœ… æ‰€æœ‰æ”»å‡»URLå·²åˆå¹¶: {len(all_attack_urls)}æ¡ â†’ å»é‡å{len(attack_unique)}æ¡")
        print(f"   â†’ {attack_output}\n")

    # å¤„ç†æµ‹è¯•é›†
    print(f"{'=' * 60}")
    print(f"ğŸ“¦ å¤„ç†æµ‹è¯•é›†:")
    print(f"{'=' * 60}\n")

    test_file = os.path.join(test_dir, "test.csv")
    print(f"ğŸ” æ£€æŸ¥æ–‡ä»¶: {os.path.abspath(test_file)}")

    if os.path.exists(test_file):
        test_output = os.path.join(output_dir, "test_urls.txt")
        test_count = process_csv_file(test_file, test_output, "æµ‹è¯•é›†")
    else:
        print(f"âš ï¸ æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨")
        test_count = 0

    # æ€»ç»“
    print(f"\n{'=' * 60}")
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"{'=' * 60}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   - æ­£å¸¸URL: {len(all_normal_urls)}æ¡ (å»é‡å{len(set(all_normal_urls))}æ¡)")
    print(f"   - æ”»å‡»URL: {len(all_attack_urls)}æ¡ (å»é‡å{len(set(all_attack_urls))}æ¡)")
    print(f"   - æµ‹è¯•URL: {test_count}æ¡")
    print(f"{'=' * 60}\n")


if __name__ == "__main__":
    main()