import json
import os
import argparse
from time import perf_counter
from src.until.config_loader import load_config
from src.models.qwen_model import QwenModel
from src.analyzer.response_analyse import ResponseAnalyzer
from src.analyzer.hybrid_detector import HybridDetector
from src.analyzer.deep_analyzer import DeepAnalyzer
from src.rules.rule_loader import load_rule_engine
from src.until.until import process_file
from src.analyzer.result_statistics import (
    analyze_results,
    print_stage2_statistics,
    print_two_stage_summary,
    print_file_time_statistics
)


def main():
    """ä¸»å‡½æ•°ï¼šä¸¤é˜¶æ®µæ£€æµ‹æµç¨‹"""

    # ========== è§£æå‘½ä»¤è¡Œå‚æ•° ==========
    parser = argparse.ArgumentParser(
        description="URLå®‰å…¨æ£€æµ‹ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  1. å®Œæ•´æ£€æµ‹ï¼ˆåŒ…å«æ·±åº¦åˆ†æï¼‰:
     python main.py

  2. ä»…ç¬¬ä¸€é˜¶æ®µæ£€æµ‹ï¼ˆè·³è¿‡æ·±åº¦åˆ†æï¼‰:
     python main.py --skip-deep-analysis

  3. ä»…ç¬¬äºŒé˜¶æ®µæ·±åº¦åˆ†æï¼ˆéœ€å·²æœ‰ç¬¬ä¸€é˜¶æ®µç»“æœï¼‰:
     python main.py --deep-analysis-only

  4. ä½¿ç”¨è‡ªå®šä¹‰é…ç½®:
     python main.py --config custom_config.yaml
        """
    )

    parser.add_argument(
        '--skip-deep-analysis',
        action='store_true',
        help='è·³è¿‡ç¬¬äºŒé˜¶æ®µæ·±åº¦åˆ†æï¼ˆä»…æ‰§è¡Œå¿«é€Ÿæ£€æµ‹ï¼‰'
    )

    parser.add_argument(
        '--deep-analysis-only',
        action='store_true',
        help='ä»…æ‰§è¡Œç¬¬äºŒé˜¶æ®µæ·±åº¦åˆ†æï¼ˆéœ€å­˜åœ¨ç¬¬ä¸€é˜¶æ®µç»“æœæ–‡ä»¶ï¼‰'
    )

    parser.add_argument(
        '--config', '-c',
        type=str,
        default='./config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./config.yaml)'
    )

    args = parser.parse_args()

    # ========== åˆå§‹åŒ– ==========
    config = load_config(args.config)

    model = QwenModel(
        model_path=config['model']['path'],
        config=config,
        dtype=config['model']['dtype']
    )

    parser_analyzer = ResponseAnalyzer()

    # ========== ç‹¬ç«‹è¿è¡Œç¬¬äºŒé˜¶æ®µ ==========
    if args.deep_analysis_only:
        print(f"\n{'=' * 60}")
        print(f"ğŸ§  ç‹¬ç«‹è¿è¡Œï¼šç¬¬äºŒé˜¶æ®µæ·±åº¦åˆ†æ")
        print(f"{'=' * 60}\n")

        output_dir = config['output']['dir']
        stage1_file = os.path.join(output_dir, config['output']['stage1_all'])

        if not os.path.exists(stage1_file):
            print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ç¬¬ä¸€é˜¶æ®µç»“æœæ–‡ä»¶: {stage1_file}")
            print(f"   è¯·å…ˆè¿è¡Œ python main-2.py ç”Ÿæˆç¬¬ä¸€é˜¶æ®µç»“æœ")
            return

        print(f"ğŸ“‚ åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœ: {stage1_file}")
        with open(stage1_file, 'r', encoding='utf-8') as f:
            all_stage1_results = json.load(f)

        # ç­›é€‰å¼‚å¸¸URL (predicted == "1")
        anomalous_results = [r for r in all_stage1_results if r.get('predicted') == "1"]
        print(f"ğŸ” å‘ç° {len(anomalous_results)} ä¸ªå¼‚å¸¸URLå¾…åˆ†æ")

        if len(anomalous_results) == 0:
            print("âœ… æ— å¼‚å¸¸URLï¼Œæ— éœ€æ·±åº¦åˆ†æ")
            return

        # æ‰§è¡Œæ·±åº¦åˆ†æ
        deep_analyzer = DeepAnalyzer(model, parser_analyzer, config)
        stage2_start = perf_counter()

        deep_results = deep_analyzer.batch_analyze(anomalous_results)

        stage2_elapsed = perf_counter() - stage2_start

        # ä¿å­˜ç¬¬äºŒé˜¶æ®µç»“æœ
        stage2_file = os.path.join(output_dir, config['output']['stage2_deep_analysis'])
        with open(stage2_file, 'w', encoding='utf-8') as f:
            json.dump(deep_results, f, ensure_ascii=False, indent=2)

        # æ‰“å°ç¬¬äºŒé˜¶æ®µç»Ÿè®¡
        print_stage2_statistics(stage2_elapsed, stage2_file, deep_results)
        return

    # ========== ç¬¬ä¸€é˜¶æ®µï¼šå®æ—¶ç›‘æµ‹ï¼ˆå¿«é€Ÿåˆ¤å®šï¼‰ ==========
    rule_engine = load_rule_engine(config.get('rules', {}))

    print(f"\n{'=' * 60}")
    print(f"âš¡ ç¬¬ä¸€é˜¶æ®µï¼šå®æ—¶ç›‘æµ‹ - å¿«é€Ÿåˆ¤å®š")
    print(f"{'=' * 60}\n")

    detector = HybridDetector(model, parser_analyzer, rule_engine, config)
    stage1_start = perf_counter()

    # ç”¨äºè®°å½•å„æ–‡ä»¶å¤„ç†æ—¶é•¿
    file_times = []

    # å¤„ç†æ­£å¸¸URL
    good_results, good_elapsed, good_filename = process_file(
        filename=config['data']['normal_file'],
        label="normal",
        query_func=detector.detect,
        data_dir=config['data']['dir']
    )
    file_times.append((good_filename, good_elapsed, len(good_results)))

    # å¤„ç†æ”»å‡»URL
    bad_results, bad_elapsed, bad_filename = process_file(
        filename=config['data']['attack_file'],
        label="attack",
        query_func=detector.detect,
        data_dir=config['data']['dir']
    )
    file_times.append((bad_filename, bad_elapsed, len(bad_results)))

    all_stage1_results = good_results + bad_results
    stage1_elapsed = perf_counter() - stage1_start

    # ç­›é€‰å¼‚å¸¸URL
    anomalous_results = [r for r in all_stage1_results if r['predicted'] == "1"]

    # ä¿å­˜å¼‚å¸¸URLåˆ—è¡¨ï¼ˆç”¨äºç¬¬äºŒé˜¶æ®µï¼‰
    output_dir = config['output']['dir']
    os.makedirs(output_dir, exist_ok=True)

    stage1_anomalous_file = os.path.join(output_dir, config['output']['stage1_anomalous'])
    with open(stage1_anomalous_file, 'w', encoding='utf-8') as f:
        for item in anomalous_results:
            f.write(f"{item['url']}\n")

    print(f"ğŸ’¾ å¼‚å¸¸URLåˆ—è¡¨å·²ä¿å­˜: {stage1_anomalous_file}")

    # ========== ç¬¬äºŒé˜¶æ®µï¼šç¦»çº¿æ·±åº¦åˆ†æï¼ˆå¯é€‰ï¼‰ ==========
    if args.skip_deep_analysis:
        print(f"\n{'=' * 60}")
        print(f"â­ï¸  è·³è¿‡ç¬¬äºŒé˜¶æ®µæ·±åº¦åˆ†æ")
        print(f"{'=' * 60}")
        print(f"ğŸ’¡ æç¤º: å¦‚éœ€æ·±åº¦åˆ†æï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤:")
        print(f"   python main-2.py --deep-analysis-only")
        print(f"{'=' * 60}\n")

        # æ‰“å°æ–‡ä»¶æ—¶é•¿ç»Ÿè®¡
        print_file_time_statistics(file_times)

        # åªè¿›è¡Œç¬¬ä¸€é˜¶æ®µè¯„ä¼°
        analyze_results(all_stage1_results, config['output'], stage1_elapsed)

    else:
        if len(anomalous_results) == 0:
            print("\nâœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸URLï¼Œè·³è¿‡ç¬¬äºŒé˜¶æ®µåˆ†æ")

            # æ‰“å°æ–‡ä»¶æ—¶é•¿ç»Ÿè®¡
            print_file_time_statistics(file_times)

            # è¿›è¡Œç¬¬ä¸€é˜¶æ®µè¯„ä¼°
            analyze_results(all_stage1_results, config['output'], stage1_elapsed)
        else:
            # æ‰§è¡Œæ·±åº¦åˆ†æ
            deep_analyzer = DeepAnalyzer(model, parser_analyzer, config)
            stage2_start = perf_counter()

            deep_results = deep_analyzer.batch_analyze(anomalous_results)

            stage2_elapsed = perf_counter() - stage2_start

            # ä¿å­˜ç¬¬äºŒé˜¶æ®µç»“æœ
            stage2_file = os.path.join(output_dir, config['output']['stage2_deep_analysis'])
            with open(stage2_file, 'w', encoding='utf-8') as f:
                json.dump(deep_results, f, ensure_ascii=False, indent=2)

            # æ‰“å°ç¬¬äºŒé˜¶æ®µç»Ÿè®¡
            print_stage2_statistics(stage2_elapsed, stage2_file, deep_results)

            # æ‰“å°ä¸¤é˜¶æ®µæ€»ç»“
            print_two_stage_summary(stage1_elapsed, stage2_elapsed)

            # æ‰“å°æ–‡ä»¶æ—¶é•¿ç»Ÿè®¡
            print_file_time_statistics(file_times)

            # è¿›è¡Œç¬¬ä¸€é˜¶æ®µè¯¦ç»†è¯„ä¼°
            analyze_results(all_stage1_results, config['output'], stage1_elapsed)


if __name__ == "__main__":
    main()