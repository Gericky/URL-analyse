import json
import os
import argparse
from time import perf_counter
from src.until.config_loader import load_config
from src.models.qwen_model import QwenModel
from src.analyzer.response_analyse import ResponseAnalyzer
from src.analyzer.hybrid_detector import HybridDetector
from src.analyzer.deep_analyzer import DeepAnalyzer
from src.rules.rule_loader import load_rule_engine
from src.until.until import process_file
from src.analyzer.offline_analysis import analyze_results


def main():
    """ä¸»å‡½æ•°ï¼šä¸¤é˜¶æ®µæ£€æµ‹æµç¨‹"""
    
    # ========== è§£æå‘½ä»¤è¡Œå‚æ•° ==========
    parser = argparse.ArgumentParser(
        description="URLå®‰å…¨æ£€æµ‹ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  1. å®Œæ•´æ£€æµ‹ï¼ˆåŒ…å«æ·±åº¦åˆ†æï¼‰:
     python main.py
  
  2. ä»…ç¬¬ä¸€é˜¶æ®µæ£€æµ‹ï¼ˆè·³è¿‡æ·±åº¦åˆ†æï¼‰:
     python main.py --skip-deep-analysis
  
  3. ä½¿ç”¨è‡ªå®šä¹‰é…ç½®:
     python main.py --config custom_config.yaml
        """
    )
    
    parser.add_argument(
        '--skip-deep-analysis',
        action='store_true',
        help='è·³è¿‡ç¬¬äºŒé˜¶æ®µæ·±åº¦åˆ†æï¼ˆä»…æ‰§è¡Œå¿«é€Ÿæ£€æµ‹ï¼‰'
    )
    
    parser.add_argument(
        '--config', '-c',
        type=str,
        default='./config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./config.yaml)'
    )
    
    args = parser.parse_args()
    
    # ========== åˆå§‹åŒ– ==========
    config = load_config(args.config)
    model = QwenModel(
        model_path=config['model']['path'],
        dtype=config['model']['dtype']
    )
    parser_analyzer = ResponseAnalyzer()
    rule_engine = load_rule_engine(config.get('rules', {}))
    
    # ========== ç¬¬ä¸€é˜¶æ®µï¼šå®æ—¶ç›‘æµ‹ï¼ˆå¿«é€Ÿåˆ¤å®šï¼‰ ==========
    print(f"\n{'='*60}")
    print(f"âš¡ ç¬¬ä¸€é˜¶æ®µï¼šå®æ—¶ç›‘æµ‹ - å¿«é€Ÿåˆ¤å®š")
    print(f"{'='*60}\n")
    
    detector = HybridDetector(model, parser_analyzer, rule_engine, config)
    stage1_start = perf_counter()
    
    # å¤„ç†æ­£å¸¸URL
    good_results = process_file(
        filename=config['data']['normal_file'],
        label="normal",
        query_func=detector.detect,
        data_dir=config['data']['dir']
    )
    
    # å¤„ç†æ”»å‡»URL
    bad_results = process_file(
        filename=config['data']['attack_file'],
        label="attack",
        query_func=detector.detect,
        data_dir=config['data']['dir']
    )
    
    all_stage1_results = good_results + bad_results
    stage1_elapsed = perf_counter() - stage1_start
    
    # ç»Ÿè®¡ç¬¬ä¸€é˜¶æ®µç»“æœ
    anomalous_results = [r for r in all_stage1_results if r['predicted'] == "1"]
    normal_results = [r for r in all_stage1_results if r['predicted'] == "0"]
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {stage1_elapsed:.2f} ç§’")
    print(f"âœ… æ­£å¸¸URL: {len(normal_results)} ä¸ª")
    print(f"âš ï¸  å¼‚å¸¸URL: {len(anomalous_results)} ä¸ª")
    print(f"{'='*60}\n")
    
    # ä¿å­˜ç¬¬ä¸€é˜¶æ®µç»“æœ
    output_dir = config['output']['dir']
    os.makedirs(output_dir, exist_ok=True)
    
    stage1_all_file = os.path.join(output_dir, config['output']['stage1_all'])
    with open(stage1_all_file, 'w', encoding='utf-8') as f:
        json.dump(all_stage1_results, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜å¼‚å¸¸URLåˆ—è¡¨ï¼ˆç”¨äºç¬¬äºŒé˜¶æ®µï¼‰
    stage1_anomalous_file = os.path.join(output_dir, config['output']['stage1_anomalous'])
    with open(stage1_anomalous_file, 'w', encoding='utf-8') as f:
        for item in anomalous_results:
            f.write(f"{item['url']}\n")
    
    print(f"ğŸ’¾ ç¬¬ä¸€é˜¶æ®µç»“æœå·²ä¿å­˜: {stage1_all_file}")
    print(f"ğŸ’¾ å¼‚å¸¸URLåˆ—è¡¨å·²ä¿å­˜: {stage1_anomalous_file}\n")
    
    # ========== ç¬¬äºŒé˜¶æ®µï¼šç¦»çº¿æ·±åº¦åˆ†æï¼ˆå¯é€‰ï¼‰ ==========
    if args.skip_deep_analysis:
        print(f"\n{'='*60}")
        print(f"â­ï¸  è·³è¿‡ç¬¬äºŒé˜¶æ®µæ·±åº¦åˆ†æ")
        print(f"{'='*60}")
        print(f"ğŸ’¡ æç¤º: å¦‚éœ€æ·±åº¦åˆ†æï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤:")
        print(f"   python deep_analysis.py --input {stage1_all_file}")
        print(f"{'='*60}\n")
    else:
        if len(anomalous_results) == 0:
            print("âœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸URLï¼Œè·³è¿‡ç¬¬äºŒé˜¶æ®µåˆ†æ")
        else:
            deep_analyzer = DeepAnalyzer(model, parser_analyzer, config)
            stage2_start = perf_counter()
            
            deep_results = deep_analyzer.batch_analyze(anomalous_results)
            
            stage2_elapsed = perf_counter() - stage2_start
            
            # ä¿å­˜ç¬¬äºŒé˜¶æ®µç»“æœ
            stage2_file = os.path.join(output_dir, config['output']['stage2_deep_analysis'])
            with open(stage2_file, 'w', encoding='utf-8') as f:
                json.dump(deep_results, f, ensure_ascii=False, indent=2)
            
            print(f"\n{'='*60}")
            print(f"ğŸ“Š ç¬¬äºŒé˜¶æ®µç»Ÿè®¡")
            print(f"{'='*60}")
            print(f"â±ï¸  æ€»ç”¨æ—¶: {stage2_elapsed:.2f} ç§’")
            print(f"ğŸ“„ æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {stage2_file}")
            print(f"{'='*60}\n")
            
            # ========== æ€»ç»“ ==========
            total_elapsed = stage1_elapsed + stage2_elapsed
            print(f"{'='*60}")
            print(f"ğŸ¯ ä¸¤é˜¶æ®µæ£€æµ‹å®Œæˆ")
            print(f"{'='*60}")
            print(f"â±ï¸  ç¬¬ä¸€é˜¶æ®µç”¨æ—¶: {stage1_elapsed:.2f} ç§’")
            print(f"â±ï¸  ç¬¬äºŒé˜¶æ®µç”¨æ—¶: {stage2_elapsed:.2f} ç§’")
            print(f"â±ï¸  æ€»ç”¨æ—¶: {total_elapsed:.2f} ç§’")
            print(f"{'='*60}\n")
    
    # ========== è¯„ä¼°æŒ‡æ ‡ ==========
    analyze_results(all_stage1_results, good_results, bad_results, config['output'])


if __name__ == "__main__":
    main()