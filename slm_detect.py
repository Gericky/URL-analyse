#ä»¥0ä»£è¡¨æ­£å¸¸URLï¼Œä»¥1ä»£è¡¨å¼‚å¸¸URL
import os
import torch
import json
import re
from transformers import AutoModelForCausalLM, AutoTokenizer
from time import perf_counter

# === é…ç½® ===
MODEL_PATH = "./Qwen3-0.6B"  # æœ¬åœ°æ¨¡å‹è·¯å¾„
DATA_DIR = "./data"          # æ•°æ®æ–‡ä»¶ç›®å½•

# === åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer ===
print("ğŸš€ æ­£åœ¨ä»æœ¬åœ°åŠ è½½ Qwen3-0.6B æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {model.device}\n")

# === è§£ææ¨¡å‹å›ç­”ï¼šå¾—åˆ°æ¨¡å‹é¢„æµ‹(0/1)å’Œç†ç”± ===
def analyze_response(response: str):
    """
    ä»æ¨¡å‹çš„ response ä¸­æŠ½å–ï¼š
      - predicted: æ¨¡å‹åˆ¤å®š "0"(æ­£å¸¸)/"1"(å¼‚å¸¸)
      - reason: ä»…ä¿ç•™ç†ç”±æ–‡æœ¬ï¼ˆå»æ‰å¼€å¤´çš„"0/1"ç­‰ï¼‰ï¼Œå¦‚æœæ¨¡å‹æ²¡ç»™ç†ç”±åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
    è§£æé€»è¾‘å°½é‡é²æ£’ï¼šä¼˜å…ˆåŒ¹é…å¼€å¤´çš„æ˜ç¡®å›ç­”ï¼Œå¦åˆ™æ ¹æ®å…³é”®è¯åˆ¤æ–­ã€‚
    """
    text = response.strip()
    # ä¼˜å…ˆåŒ¹é…å¼€å¤´æ˜ç¡®çš„ "0" / "1" æˆ– "å›ç­”: 0/1"
    m = re.match(r'^\s*(å›ç­”[:ï¼š]\s*)?(0|1)\s*[\u3000\s,ï¼Œ\.ã€‚.:\-ï¼š]*([\s\S]*)', text)
    if m:
        pred = m.group(2)  # "0" æˆ– "1"
        reason = m.group(3).strip()
        # å¦‚æœç†ç”±ä¸­ä»ç„¶å«æœ‰ã€Œ0/1ã€å¼€å¤´ï¼Œå†å»æ‰
        reason = re.sub(r'^(0|1)[\sï¼Œã€‚:ï¼š,-]*', '', reason).strip()
        return pred, reason

    # è‹¥æ— æ˜ç¡®å¼€å¤´ï¼Œç”¨å…³é”®å­—åˆ¤æ–­ï¼ˆå»æ‰ç©ºæ ¼å’Œä¸­æ–‡æ ‡ç‚¹ä¾¿äºåŒ¹é…ï¼‰
    compact = re.sub(r'[\sï¼Œã€‚ã€""]', '', text)
    if re.search(r'(ä¸æ˜¯æ”»å‡»|éæ”»å‡»|å®‰å…¨|æ­£å¸¸)', compact):
        pred = '0'
    elif re.search(r'(æ˜¯æ”»å‡»|å±äºæ”»å‡»|æ¶æ„|å¼‚å¸¸|SQLæ³¨å…¥|XSS|å‘½ä»¤æ³¨å…¥|æ”»å‡»)', compact):
        pred = '1'
    else:
        # é»˜è®¤ä¿å®ˆåˆ¤æ–­ä¸ºæ­£å¸¸
        pred = '0'

    # æ¸…ç†ç†ç”±ï¼šå»æ‰é¦–éƒ¨çš„"0/1"ç­‰ï¼Œå†å°½é‡ä¿ç•™åç»­æ–‡å­—
    reason = re.sub(r'^(å›ç­”[:ï¼š]\s*)?(0|1)[\sï¼Œã€‚:ï¼š,-]*', '', text).strip()
    return pred, reason

# === ä¸»æ£€æµ‹å‡½æ•°ï¼šè¿”å›æ¨¡å‹é¢„æµ‹ã€ç†ç”±ã€ç”¨æ—¶ç­‰ä¿¡æ¯ ===
def query_model_for_url(url: str) -> dict:
    messages = [
    {
        "role": "system",
        "content": (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Webå®‰å…¨æ£€æµ‹ç³»ç»Ÿï¼Œè´Ÿè´£åˆ†æURLè¯·æ±‚å¹¶è¯†åˆ«æ½œåœ¨çš„å®‰å…¨å¨èƒã€‚"
            "ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç»™å®šçš„URLæ˜¯å¦åŒ…å«æ¶æ„æ”»å‡»ç‰¹å¾ï¼Œå¹¶ç»™å‡ºæ˜ç¡®çš„åˆ†ç±»ç»“æœã€‚"
            "åˆ¤å®šè§„åˆ™ï¼š"
            "- è¾“å‡º 0ï¼šè¡¨ç¤ºæ­£å¸¸URLï¼Œå³å®‰å…¨çš„ã€åˆæ³•çš„Webè¯·æ±‚"
            "- è¾“å‡º 1ï¼šè¡¨ç¤ºå¼‚å¸¸URLï¼Œå³åŒ…å«æ”»å‡»ç‰¹å¾çš„æ¶æ„è¯·æ±‚"
            "æ”»å‡»ç‰¹å¾åŒ…æ‹¬ä½†ä¸é™äºï¼š"
            "â€¢ SQLæ³¨å…¥ï¼šåŒ…å« `'`ã€`--`ã€`#`ã€`union`ã€`select` ç­‰SQLè¯­å¥"
            "â€¢ XSSè·¨ç«™è„šæœ¬ï¼šåŒ…å« `<script>`ã€`</script>`ã€`javascript:`ã€`onerror=` ç­‰"
            "â€¢ å‘½ä»¤æ³¨å…¥ï¼šåŒ…å« `|`ã€`;`ã€`&&`ã€shellå‘½ä»¤ç­‰"
            "â€¢ è·¯å¾„éå†ï¼šåŒ…å« `../`ã€`..\\`ã€`/etc/passwd` ç­‰"
            "â€¢ æ–‡ä»¶åŒ…å«ï¼šåŒ…å« `<?php`ã€`include`ã€è¿œç¨‹æ–‡ä»¶è·¯å¾„ç­‰"
            "â€¢ å…¶ä»–æ¶æ„ç‰¹å¾ï¼šç¼–ç ç»•è¿‡ã€å¼‚å¸¸å­—ç¬¦ã€æ•æ„Ÿè·¯å¾„è®¿é—®ç­‰"
            "æ­£å¸¸è¯·æ±‚ç‰¹å¾ï¼š"
            "â€¢ ä»…åŒ…å«å¸¸è§„è·¯å¾„å’Œé™æ€èµ„æºè®¿é—®"
            "â€¢ å‚æ•°å€¼ä¸ºæ­£å¸¸çš„ä¸šåŠ¡æ•°æ®ï¼Œæ— æ³¨å…¥ç¬¦å·"
            "â€¢ ç¬¦åˆæ ‡å‡†çš„HTTPè¯·æ±‚æ ¼å¼"
            "è¯·åŸºäºOWASP Top 10å®‰å…¨æ ‡å‡†è¿›è¡Œåˆ¤æ–­ï¼Œä¿æŒé«˜çµæ•åº¦ä½†é¿å…è¯¯æŠ¥ã€‚"
        )
    },
    {
        "role": "user",
        "content": (
            "è¯·åˆ†æä»¥ä¸‹URLè¯·æ±‚ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¸ºæ¶æ„æ”»å‡»ï¼š\n"
            f"URL: {url}\n\n"
            "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n"
            "é¦–å…ˆè¾“å‡ºåˆ†ç±»ç»“æœï¼ˆ0æˆ–1ï¼‰ï¼š\n"
            "- 0 è¡¨ç¤ºæ­£å¸¸URL\n"
            "- 1 è¡¨ç¤ºå¼‚å¸¸URL\n"
            "ç„¶åè¯´æ˜åˆ¤æ–­ç†ç”±ï¼ŒåŒ…æ‹¬ï¼š\n"
            "- å¦‚æœæ˜¯å¼‚å¸¸URLï¼ˆ1ï¼‰ï¼Œè¯·æŒ‡å‡ºå…·ä½“çš„æ”»å‡»ç±»å‹ï¼ˆå¦‚SQLæ³¨å…¥ã€XSSã€å‘½ä»¤æ‰§è¡Œã€è·¯å¾„éå†ç­‰ï¼‰å’Œæ¶æ„ç‰¹å¾\n"
            "- å¦‚æœæ˜¯æ­£å¸¸URLï¼ˆ0ï¼‰ï¼Œè¯·è¯´æ˜ä¸ºä½•åˆ¤å®šä¸ºå®‰å…¨è¯·æ±‚\n\n"
            "è¯·ç›´æ¥ä»¥æ•°å­—0æˆ–1å¼€å¤´å›ç­”ã€‚"
        )
    }
]

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    start_time = perf_counter()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=False,
            temperature=0.0,
            pad_token_id=tokenizer.eos_token_id
        )
    end_time = perf_counter()

    raw_response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
    predicted, reason = analyze_response(raw_response)
    elapsed = round(end_time - start_time, 3)

    return {
        "url": url,
        "predicted": predicted,             # æ¨¡å‹åˆ¤å®šï¼ˆ"0"/"1"ï¼‰
        "reason": reason,                   # ä»…ç†ç”±ï¼ˆä¸åŒ…å«"0/1"ï¼‰
        "elapsed_time_sec": elapsed,
        "raw_response": raw_response        # ä¿ç•™åŸå§‹å®Œæ•´å›å¤ä»¥ä¾›è°ƒè¯•
    }

# === æ‰¹é‡å¤„ç†æ–‡ä»¶ ===
def process_file(filename, label):
    filepath = os.path.join(DATA_DIR, filename)
    if not os.path.exists(filepath):
        print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {filepath}")
        return []
    with open(filepath, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]
    print(f"\nğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}ï¼Œå…± {len(lines)} æ¡")
    file_start = perf_counter()
    results = []
    for i, url in enumerate(lines, 1):
        print(f"[{label}] ç¬¬ {i}/{len(lines)}: {url}")
        res = query_model_for_url(url)
        # æŠŠçœŸå®æ ‡ç­¾å†™è¿›å»ï¼ˆ0ä¸ºæ­£å¸¸ï¼Œ1ä¸ºæ”»å‡»ï¼‰
        res["true_label"] = "1" if label == "attack" else "0"
        results.append(res)
        print(f"  æ¨¡å‹åˆ¤å®š: {res['predicted']} | çœŸå®æ ‡ç­¾: {res['true_label']} | ç”¨æ—¶: {res['elapsed_time_sec']}s")
        print(f"  ç†ç”±ï¼ˆç®€è¦ï¼‰: {res['reason']}\n")
    file_elapsed = perf_counter() - file_start
    print(f"â±ï¸ æ–‡ä»¶ {filename} æ€»ç”¨æ—¶: {file_elapsed:.2f} ç§’\n")
    return results

if __name__ == "__main__":
    total_start = perf_counter()

    good_results = process_file("good-10.txt", "normal")
    bad_results = process_file("bad-10.txt", "attack")

    all_results = good_results + bad_results

    total_elapsed = perf_counter() - total_start
    print(f"ğŸ¯ å…¨éƒ¨æ£€æµ‹å®Œæˆï¼Œæ€»ç”¨æ—¶ {total_elapsed:.2f} ç§’")
    
    # è®¡ç®—å‡†ç¡®ç‡ç»Ÿè®¡
    correct = sum(1 for r in all_results if r['predicted'] == r['true_label'])
    total = len(all_results)
    accuracy = (correct / total * 100) if total > 0 else 0
    print(f"ğŸ“Š å‡†ç¡®ç‡: {correct}/{total} = {accuracy:.2f}%")
    # è®¡ç®—å¬å›ç‡
    relevant = sum(1 for r in all_results if r['true_label'] == "1")
    retrieved = sum(1 for r in all_results if r['predicted'] == "1")
    recall = (retrieved / relevant * 100) if relevant > 0 else 0
    print(f"ğŸ“ˆ å¬å›ç‡: {retrieved}/{relevant} = {recall:.2f}%")

    # ä¿å­˜ç»“æœ
    out_all = "./output/slm_results_10_all.json"
    out_good = "./output/slm_results_10_good.json"
    out_bad = "./output/slm_results_10_bad.json"

    with open(out_all, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    with open(out_good, "w", encoding="utf-8") as f:
        json.dump(good_results, f, ensure_ascii=False, indent=2)
    with open(out_bad, "w", encoding="utf-8") as f:
        json.dump(bad_results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜ï¼š{out_all}, {out_good}, {out_bad}")