import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from time import perf_counter

class QwenModel:
    def __init__(self, model_path, dtype="float16"):
        """åˆå§‹åŒ–Qwenæ¨¡å‹"""
        print(f"ğŸš€ æ­£åœ¨ä»æœ¬åœ°åŠ è½½ Qwen æ¨¡å‹: {model_path}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        # è®¾ç½®dtype
        torch_dtype = torch.float16 if dtype == "float16" else torch.float32
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            dtype=torch_dtype,
            device_map="auto",
            trust_remote_code=True
        )
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.model.device}\n")
    
    def query(self, url: str, max_new_tokens=128, temperature=0.0) -> dict:
        """æŸ¥è¯¢æ¨¡å‹åˆ¤æ–­URL"""
        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Webå®‰å…¨æ£€æµ‹ç³»ç»Ÿ,è´Ÿè´£åˆ†æURLè¯·æ±‚å¹¶è¯†åˆ«æ½œåœ¨çš„å®‰å…¨å¨èƒã€‚"
                    "ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç»™å®šçš„URLæ˜¯å¦åŒ…å«æ¶æ„æ”»å‡»ç‰¹å¾,å¹¶ç»™å‡ºæ˜ç¡®çš„åˆ†ç±»ç»“æœã€‚"
                    "åˆ¤å®šè§„åˆ™:"
                    "- è¾“å‡º 0:è¡¨ç¤ºæ­£å¸¸URL,å³å®‰å…¨çš„ã€åˆæ³•çš„Webè¯·æ±‚"
                    "- è¾“å‡º 1:è¡¨ç¤ºå¼‚å¸¸URL,å³åŒ…å«æ”»å‡»ç‰¹å¾çš„æ¶æ„è¯·æ±‚"
                    "æ”»å‡»ç‰¹å¾åŒ…æ‹¬ä½†ä¸é™äº:"
                    "â€¢ SQLæ³¨å…¥:åŒ…å« `'`ã€`--`ã€`#`ã€`union`ã€`select` ç­‰SQLè¯­å¥"
                    "â€¢ XSSè·¨ç«™è„šæœ¬:åŒ…å« `<script>`ã€`</script>`ã€`javascript:`ã€`onerror=` ç­‰"
                    "â€¢ å‘½ä»¤æ³¨å…¥:åŒ…å« `|`ã€`;`ã€`&&`ã€shellå‘½ä»¤ç­‰"
                    "â€¢ è·¯å¾„éå†:åŒ…å« `../`ã€`..\\`ã€`/etc/passwd` ç­‰"
                    "â€¢ æ–‡ä»¶åŒ…å«:åŒ…å« `<?php`ã€`include`ã€è¿œç¨‹æ–‡ä»¶è·¯å¾„ç­‰"
                    "â€¢ å…¶ä»–æ¶æ„ç‰¹å¾:ç¼–ç ç»•è¿‡ã€å¼‚å¸¸å­—ç¬¦ã€æ•æ„Ÿè·¯å¾„è®¿é—®ç­‰"
                    "æ­£å¸¸è¯·æ±‚ç‰¹å¾:"
                    "â€¢ ä»…åŒ…å«å¸¸è§„è·¯å¾„å’Œé™æ€èµ„æºè®¿é—®"
                    "â€¢ å‚æ•°å€¼ä¸ºæ­£å¸¸çš„ä¸šåŠ¡æ•°æ®,æ— æ³¨å…¥ç¬¦å·"
                    "â€¢ ç¬¦åˆæ ‡å‡†çš„HTTPè¯·æ±‚æ ¼å¼"
                    "è¯·åŸºäºOWASP Top 10å®‰å…¨æ ‡å‡†è¿›è¡Œåˆ¤æ–­,ä¿æŒé«˜çµæ•åº¦ä½†é¿å…è¯¯æŠ¥ã€‚"
                )
            },
            {
                "role": "user",
                "content": (
                    "è¯·åˆ†æä»¥ä¸‹URLè¯·æ±‚,åˆ¤æ–­å…¶æ˜¯å¦ä¸ºæ¶æ„æ”»å‡»:\n"
                    f"URL: {url}\n\n"
                    "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”:\n"
                    "é¦–å…ˆè¾“å‡ºåˆ†ç±»ç»“æœ(0æˆ–1):\n"
                    "- 0 è¡¨ç¤ºæ­£å¸¸URL\n"
                    "- 1 è¡¨ç¤ºå¼‚å¸¸URL\n"
                    "ç„¶åè¯´æ˜åˆ¤æ–­ç†ç”±,åŒ…æ‹¬:\n"
                    "- å¦‚æœæ˜¯å¼‚å¸¸URL(1),è¯·æŒ‡å‡ºå…·ä½“çš„æ”»å‡»ç±»å‹(å¦‚SQLæ³¨å…¥ã€XSSã€å‘½ä»¤æ‰§è¡Œã€è·¯å¾„éå†ç­‰)å’Œæ¶æ„ç‰¹å¾\n"
                    "- å¦‚æœæ˜¯æ­£å¸¸URL(0),è¯·è¯´æ˜ä¸ºä½•åˆ¤å®šä¸ºå®‰å…¨è¯·æ±‚\n\n"
                    "è¯·ç›´æ¥ä»¥æ•°å­—0æˆ–1å¼€å¤´å›ç­”ã€‚"
                )
            }
        ]
        
        text = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True, 
            enable_thinking=False
        )
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        start_time = perf_counter()
        with torch.no_grad():
            # æ ¹æ®temperatureå†³å®šé‡‡æ ·ç­–ç•¥
            if temperature > 0:
                # ä½¿ç”¨é‡‡æ ·æ¨¡å¼
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=temperature,
                    top_p=0.9,
                    top_k=50,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            else:
                # ä½¿ç”¨è´ªå©ªè§£ç æ¨¡å¼(ç¡®å®šæ€§è¾“å‡º)
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
        end_time = perf_counter()
        
        raw_response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        elapsed = round(end_time - start_time, 3)
        
        # è¿”å›åŸå§‹å“åº”,ç”±å¤–éƒ¨è§£æ
        return {
            "url": url,
            "raw_response": raw_response,
            "elapsed_time_sec": elapsed
        }