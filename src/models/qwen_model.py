import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from time import perf_counter
import os


class QwenModel:
    def __init__(self, model_path, config, dtype="float16"):
        """
        åˆå§‹åŒ–Qwenæ¨¡åž‹
        
        Args:
            model_path: æ¨¡åž‹è·¯å¾„
            config: å®Œæ•´é…ç½®å­—å…¸
            dtype: æ•°æ®ç±»åž‹
        """
        print(f"ðŸš€ æ­£åœ¨ä»Žæœ¬åœ°åŠ è½½ Qwen æ¨¡åž‹: {model_path}...")
        
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        dtype = torch.float16 if dtype == "float16" else torch.float32
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            dtype=dtype,
            device_map="auto",
            trust_remote_code=True
        )
        print(f"âœ… æ¨¡åž‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.model.device}\n")
        
        # âœ¨ åŠ è½½æç¤ºè¯æ¨¡æ¿
        self._load_prompts()
    
    def _load_prompts(self):
        """ä»Žé…ç½®æ–‡ä»¶æŒ‡å®šçš„è·¯å¾„åŠ è½½æç¤ºè¯æ¨¡æ¿"""
        # åŠ è½½å¿«é€Ÿæ£€æµ‹æç¤ºè¯
        fast_prompt_path = self.config['model']['fast_detection'].get('prompt', '')
        if fast_prompt_path and os.path.exists(fast_prompt_path):
            with open(fast_prompt_path, 'r', encoding='utf-8') as f:
                self.fast_detection_prompt = f.read().strip()
            print(f"âœ… å·²åŠ è½½å¿«é€Ÿæ£€æµ‹æç¤ºè¯: {fast_prompt_path}")
        else:
            # é™çº§ï¼šä½¿ç”¨é»˜è®¤æç¤ºè¯
            self.fast_detection_prompt = self._get_default_fast_prompt()
            print(f"âš ï¸  å¿«é€Ÿæ£€æµ‹æç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
        
        # åŠ è½½æ·±åº¦åˆ†æžæç¤ºè¯
        deep_prompt_path = self.config['model']['deep_analysis'].get('prompt', '')
        if deep_prompt_path and os.path.exists(deep_prompt_path):
            with open(deep_prompt_path, 'r', encoding='utf-8') as f:
                self.deep_analysis_prompt = f.read().strip()
            print(f"âœ… å·²åŠ è½½æ·±åº¦åˆ†æžæç¤ºè¯: {deep_prompt_path}")
        else:
            # é™çº§ï¼šä½¿ç”¨é»˜è®¤æç¤ºè¯
            self.deep_analysis_prompt = self._get_default_deep_prompt()
            print(f"âš ï¸  æ·±åº¦åˆ†æžæç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
    
    def _get_default_fast_prompt(self) -> str:
        """é»˜è®¤çš„å¿«é€Ÿæ£€æµ‹æç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªURLå®‰å…¨å¿«é€Ÿæ£€æµ‹ç³»ç»Ÿã€‚
ä»»åŠ¡ï¼šå¿«é€Ÿåˆ¤æ–­URLæ˜¯å¦ä¸ºæ”»å‡»ï¼Œåªè¾“å‡ºåˆ¤å®šç»“æžœã€‚
è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
- å¦‚æžœæ˜¯æ­£å¸¸URLï¼Œè¾“å‡ºï¼š0
- å¦‚æžœæ˜¯æ”»å‡»URLï¼Œè¾“å‡ºï¼š1|æ”»å‡»ç±»åž‹
æ”»å‡»ç±»åž‹åŒ…æ‹¬ï¼šsql_injection, xss, command_injection, path_traversal, file_inclusion, DDoS, malicious_file_access
ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šï¼Œåªè¾“å‡ºåˆ¤å®šç»“æžœã€‚"""
    
    def _get_default_deep_prompt(self) -> str:
        """é»˜è®¤çš„æ·±åº¦åˆ†æžæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€åé«˜çº§ç½‘ç»œå®‰å…¨åˆ†æžå¼•æ“Žï¼Œè´Ÿè´£å¯¹å¯ç–‘URLè¿›è¡Œæ·±åº¦å¨èƒåˆ†æžã€‚
è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
1. æ”»å‡»ç±»åž‹ï¼š[å…·ä½“ç±»åž‹]
2. ç®€è¦æ¦‚è¿°ï¼š[ä¸€å¥è¯æ¦‚æ‹¬]
3. è¡Œä¸ºæè¿°ï¼š[è¯¦ç»†æè¿°æ”»å‡»è¡Œä¸º]
4. æˆå› åˆ†æžï¼š[åˆ†æžä¸ºä½•åˆ¤å®šä¸ºæ”»å‡»]
5. åˆ¤å®šä¾æ®ï¼š[åˆ—å‡ºå…³é”®ç‰¹å¾]
6. é£Žé™©è¯„ä¼°ï¼š[è¯„ä¼°å±å®³ç¨‹åº¦]
7. é˜²æŠ¤å»ºè®®ï¼š[ç»™å‡ºé˜²æŠ¤æŽªæ–½]"""
    
    def fast_detect(self, url: str, similar_cases=None) -> dict:
        """
        ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿæ£€æµ‹æ¨¡å¼
        - åªè¾“å‡º label (0/1) å’Œæ”»å‡»ç±»åž‹
        - ä¸è¾“å‡ºè¯¦ç»†ç†ç”±
        - æ”¯æŒRAGå¢žå¼º
        
        Args:
            url: å¾…æ£€æµ‹URL
            similar_cases: RAGæ£€ç´¢çš„ç›¸ä¼¼æ¡ˆä¾‹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        """
        # âœ¨ ä»Žé…ç½®è¯»å–å‚æ•°
        max_new_tokens = self.config['model']['fast_detection'].get('max_new_tokens', 10)
        temperature = self.config['model']['fast_detection'].get('temperature', 0.0)
        
        # æž„å»ºç”¨æˆ·æç¤ºè¯
        user_prompt = f"URL: {url}\nåˆ¤å®šç»“æžœï¼š"
        
        # âœ¨ å¦‚æžœæœ‰ç›¸ä¼¼æ¡ˆä¾‹ï¼Œå¢žå¼ºæç¤ºè¯
        if similar_cases:
            rag_context = "\n\nå‚è€ƒç›¸ä¼¼æ¡ˆä¾‹:\n"
            for i, case in enumerate(similar_cases[:3], 1):
                label_cn = "æ”»å‡»" if case['label'] == 'attack' else "æ­£å¸¸"
                rag_context += f"{i}. {label_cn} (ç›¸ä¼¼åº¦ {case['similarity_score']:.1%}): {case['url'][:60]}...\n"
            user_prompt = rag_context + "\n" + user_prompt
        
        messages = [
            {
                "role": "system",
                "content": self.fast_detection_prompt
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        return self._generate(messages, max_new_tokens, temperature, url)
    
    def deep_analyze(self, url: str, attack_type: str, similar_cases=None) -> dict:
        """
        ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦åˆ†æžæ¨¡å¼
        - è¾“å‡ºè¯¦ç»†çš„å®‰å…¨åˆ†æžæŠ¥å‘Š
        - æ”¯æŒRAGå¢žå¼º
        
        Args:
            url: å¾…åˆ†æžURL
            attack_type: ç¬¬ä¸€é˜¶æ®µè¯†åˆ«çš„æ”»å‡»ç±»åž‹
            similar_cases: RAGæ£€ç´¢çš„ç›¸ä¼¼æ¡ˆä¾‹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        """
        # âœ¨ ä»Žé…ç½®è¯»å–å‚æ•°
        max_new_tokens = self.config['model']['deep_analysis'].get('max_new_tokens', 512)
        temperature = self.config['model']['deep_analysis'].get('temperature', 0.3)
        
        # æž„å»ºç”¨æˆ·æç¤ºè¯
        user_prompt = f"""è¯·å¯¹ä»¥ä¸‹URLè¿›è¡Œæ·±åº¦å®‰å…¨åˆ†æžï¼š

URL: {url}
åˆæ­¥åˆ¤å®š: {attack_type}"""
        
        # âœ¨ å¦‚æžœæœ‰ç›¸ä¼¼æ¡ˆä¾‹ï¼Œå¢žå¼ºæç¤ºè¯
        if similar_cases:
            rag_context = "\n\n### å‚è€ƒç›¸ä¼¼æ¡ˆä¾‹:\n"
            for i, case in enumerate(similar_cases[:5], 1):
                label_cn = "æ”»å‡»" if case['label'] == 'attack' else "æ­£å¸¸"
                rag_context += f"\n**æ¡ˆä¾‹{i}** (ç›¸ä¼¼åº¦: {case['similarity_score']:.2%})\n"
                rag_context += f"- URL: `{case['url'][:80]}{'...' if len(case['url']) > 80 else ''}`\n"
                rag_context += f"- ç±»åž‹: {label_cn}\n"
            user_prompt = user_prompt + rag_context + "\n\n### åˆ†æžä»»åŠ¡\nåŸºäºŽä»¥ä¸Šç›¸ä¼¼æ¡ˆä¾‹å’Œä½ çš„çŸ¥è¯†ï¼Œè¯·å¯¹ç›®æ ‡URLè¿›è¡Œæ·±åº¦åˆ†æžã€‚"
        
        messages = [
            {
                "role": "system",
                "content": self.deep_analysis_prompt
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        return self._generate(messages, max_new_tokens, temperature, url)
    
    def _generate(self, messages, max_new_tokens, temperature, url):
        """å†…éƒ¨ç”Ÿæˆæ–¹æ³•"""
        text = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True, 
            enable_thinking=False
        )
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        start_time = perf_counter()
        
        with torch.no_grad():
            if temperature > 0:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=temperature,
                    top_p=0.9,
                    top_k=50,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
        
        elapsed_time = perf_counter() - start_time
        
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        return {
            'url': url,
            'response': response,
            'elapsed_time': elapsed_time
        }