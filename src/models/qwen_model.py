import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from time import perf_counter

class QwenModel:
    def __init__(self, model_path, dtype="float16"):
        """åˆå§‹åŒ–Qwenæ¨¡åž‹"""
        print(f"ðŸš€ æ­£åœ¨ä»Žæœ¬åœ°åŠ è½½ Qwen æ¨¡åž‹: {model_path}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        torch_dtype = torch.float16 if dtype == "float16" else torch.float32
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            dtype=torch_dtype,
            device_map="auto",
            trust_remote_code=True
        )
        print(f"âœ… æ¨¡åž‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.model.device}\n")
    
    def fast_detect(self, url: str, max_new_tokens=50, temperature=0.0) -> dict:
        """
        ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿæ£€æµ‹æ¨¡å¼
        - åªè¾“å‡º label (0/1) å’Œæ”»å‡»ç±»åž‹
        - ä¸è¾“å‡ºè¯¦ç»†ç†ç”±
        """
        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªURLå®‰å…¨å¿«é€Ÿæ£€æµ‹ç³»ç»Ÿã€‚"
                    "ä»»åŠ¡ï¼šå¿«é€Ÿåˆ¤æ–­URLæ˜¯å¦ä¸ºæ”»å‡»ï¼Œåªè¾“å‡ºåˆ¤å®šç»“æžœã€‚"
                    "è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š"
                    "- å¦‚æžœæ˜¯æ­£å¸¸URLï¼Œè¾“å‡ºï¼š0"
                    "- å¦‚æžœæ˜¯æ”»å‡»URLï¼Œè¾“å‡ºï¼š1|æ”»å‡»ç±»åž‹"
                    "æ”»å‡»ç±»åž‹åŒ…æ‹¬ï¼šsql_injection, xss, command_injection, path_traversal, file_inclusion,DDoS"
                    "ç¤ºä¾‹ï¼š"
                    "  æ­£å¸¸URL â†’ 0"
                    "  SQLæ³¨å…¥ â†’ 1|sql_injection"
                    "  XSSæ”»å‡» â†’ 1|xss"
                    "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šï¼Œåªè¾“å‡ºåˆ¤å®šç»“æžœã€‚"
    #                "?dåŽé¢çš„å†…å®¹æ˜¯URLå‚æ•°ï¼Œè¯·é‡ç‚¹å…³æ³¨ã€‚"
                    
                    "æ¯”å¦‚/skypearl/cn/validatorAction.action?d=1497656987æ˜¯æ­£å¸¸URLï¼Œ"
     #               "è€Œ/skypearl/cn/validatorAction.action?d=1497656987' OR '1'='1æ˜¯SQLæ³¨å…¥æ”»å‡»ã€‚"

                )
            },
            {
                "role": "user",
                "content": f"URL: {url}\nåˆ¤å®šç»“æžœï¼š"
            }
        ]
        
        return self._generate(messages, max_new_tokens, temperature, url)
    
    def deep_analyze(self, url: str, attack_type: str, max_new_tokens=512, temperature=0.3) -> dict:
        """
        ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦åˆ†æžæ¨¡å¼
        - è¾“å‡ºè¯¦ç»†çš„ç»“æž„åŒ–åˆ†æžæŠ¥å‘Š
        """
        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªWebå®‰å…¨ä¸“å®¶ï¼Œè´Ÿè´£å¯¹æ”»å‡»URLè¿›è¡Œæ·±åº¦åˆ†æžã€‚"
                    "è¯·æŒ‰ä»¥ä¸‹ç»“æž„è¾“å‡ºè¯¦ç»†åˆ†æžæŠ¥å‘Šï¼š\n"
                    "## æ”»å‡»ç±»åž‹\n[æ”»å‡»åˆ†ç±»]\n\n"
                    "## ç®€è¦æ¦‚è¿°\n[ä¸€å¥è¯æè¿°æ”»å‡»æ„å›¾]\n\n"
                    "## è¡Œä¸ºæè¿°\n[è¯¦ç»†æè¿°URLä¸­çš„æ¶æ„è¡Œä¸º]\n\n"
                    "## æˆå› åˆ†æž\n[åˆ†æžä¸ºä½•ä¼šäº§ç”Ÿæ­¤ç±»æ”»å‡»]\n\n"
                    "## åˆ¤å®šä¾æ®\n[åˆ—å‡ºåˆ¤å®šä¸ºæ”»å‡»çš„å…³é”®ç‰¹å¾]\n\n"
                    "## é£Žé™©è¯„ä¼°\n[è¯„ä¼°æ”»å‡»çš„å±å®³ç¨‹åº¦]\n\n"
                    "## é˜²æŠ¤å»ºè®®\n[ç»™å‡ºå…·ä½“çš„é˜²æŠ¤æŽªæ–½]"
                )
            },
            {
                "role": "user",
                "content": (
                    f"è¯·å¯¹ä»¥ä¸‹æ”»å‡»URLè¿›è¡Œæ·±åº¦åˆ†æžï¼š\n"
                    f"URL: {url}\n"
                    f"åˆæ­¥åˆ¤å®šç±»åž‹: {attack_type}\n\n"
                    f"è¯·è¾“å‡ºå®Œæ•´çš„åˆ†æžæŠ¥å‘Šï¼š"
                )
            }
        ]
        
        return self._generate(messages, max_new_tokens, temperature, url)
    
    def _generate(self, messages, max_new_tokens, temperature, url):
        """å†…éƒ¨ç”Ÿæˆæ–¹æ³•"""
        text = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True, 
            enable_thinking=False
        )
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        start_time = perf_counter()
        with torch.no_grad():
            if temperature > 0:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=temperature,
                    top_p=0.9,
                    top_k=50,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
        end_time = perf_counter()
        
        raw_response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        return {
            "url": url,
            "raw_response": raw_response,
            "elapsed_time_sec": round(end_time - start_time, 3)
        }