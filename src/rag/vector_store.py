"""FAISSå‘é‡å­˜å‚¨ç®¡ç†å™¨"""
import faiss
import numpy as np
import pickle
from typing import List, Tuple, Optional
from sentence_transformers import SentenceTransformer
import os 

class VectorStore:
    """FAISSå‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, model_name: str, dimension: int):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨
        
        Args:
            model_name: SentenceTransformeræ¨¡å‹åç§°
            dimension: å‘é‡ç»´åº¦
        """
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½BGEæ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dimension = dimension
        self.index = None
        self.metadata = []
        print(f"âœ… BGEæ¨¡å‹åŠ è½½å®Œæˆ (ç»´åº¦: {dimension})")
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            np.ndarray: å‘é‡æ•°ç»„ (N, dimension)
        """
        # âœ… å½’ä¸€åŒ–å‘é‡ï¼ˆä½¿å†…ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=True,  # â† å…³é”®ï¼šå½’ä¸€åŒ–
            show_progress_bar=False
        )
        return embeddings.astype('float32')
    
    def build_index(self, texts: List[str], labels: List[str], 
                    metadata: List[dict] = None):
        """
        æ„å»ºå‘é‡ç´¢å¼•
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨ ('normal' or 'attack')
            metadata: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        """
        if len(texts) != len(labels):
            raise ValueError("æ–‡æœ¬æ•°é‡ä¸æ ‡ç­¾æ•°é‡ä¸åŒ¹é…")
        
        # 1. æ–‡æœ¬å‘é‡åŒ–
        embeddings = self.encode(texts)
        
        # 2. âœ¨ ä½¿ç”¨å†…ç§¯ç´¢å¼•ï¼ˆå¯¹å½’ä¸€åŒ–å‘é‡ç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.index = faiss.IndexFlatIP(self.dimension)
        #                  ^^^^^^^^
        #                  å†…ç§¯ç´¢å¼•ï¼ˆInner Productï¼‰
        
        # 3. æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        self.index.add(embeddings)
        
        # 4. ä¿å­˜å…ƒæ•°æ®
        self.metadata = [
            {
                'url': texts[i],
                'label': labels[i],
                'metadata': metadata[i] if metadata else {}
            }
            for i in range(len(texts))
        ]
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(texts)} æ¡å‘é‡")
    
    def search(self, query_text: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """
        æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            List[Tuple[int, float]]: [(ç´¢å¼•, ä½™å¼¦ç›¸ä¼¼åº¦), ...]
                                     ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´: [0, 1]
        """
        if self.index is None:
            raise ValueError("å‘é‡åº“æœªåˆå§‹åŒ–")
        
        # 1. æŸ¥è¯¢æ–‡æœ¬å‘é‡åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰
        query_vector = self.encode([query_text])
        
        # 2. âœ¨ FAISS æ£€ç´¢ï¼ˆè¿”å›å†…ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        similarities, indices = self.index.search(query_vector, top_k)
        #   ^^^^^^^^^^^^  
        #   å†…ç§¯åˆ†æ•°ï¼ˆå¯¹å½’ä¸€åŒ–å‘é‡ = ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        # 3. è¿”å›ç»“æœ
        results = []
        for idx, sim in zip(indices[0], similarities[0]):
            if idx != -1:  # FAISS ç”¨ -1 è¡¨ç¤ºæ— æ•ˆç»“æœ
                # âœ… ä½™å¼¦ç›¸ä¼¼åº¦å·²ç»åœ¨ [0, 1] èŒƒå›´å†…ï¼Œæ— éœ€è½¬æ¢
                results.append((int(idx), float(sim)))
        
        return results
    
    def save(self, index_path: str, metadata_path: str):
        """ä¿å­˜å‘é‡åº“å’Œå…ƒæ•°æ®"""
        if self.index is None:
            raise ValueError("å‘é‡åº“æœªåˆå§‹åŒ–")
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, index_path)
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.metadata, f)
        
        print(f"ğŸ’¾ å‘é‡åº“å·²ä¿å­˜:")
        print(f"   ç´¢å¼•: {index_path}")
        print(f"   å…ƒæ•°æ®: {metadata_path}")
    
    def load(self, index_path: str, metadata_path: str):
        """åŠ è½½å‘é‡åº“å’Œå…ƒæ•°æ®"""
        # åŠ è½½FAISSç´¢å¼•
        self.index = faiss.read_index(index_path)
        
        # åŠ è½½å…ƒæ•°æ®
        with open(metadata_path, 'rb') as f:
            self.metadata = pickle.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½å‘é‡åº“: {len(self.metadata)} æ¡è®°å½•")

    def add_url_history_folder(self, history_folder: str):
        """
        ä»æ–‡ä»¶å¤¹åŠ è½½URLå†å²ï¼ˆæŒ‰æ”»å‡»ç±»å‹åˆ†ç±»ï¼‰
        
        Args:
            history_folder: URLå†å²æ–‡ä»¶å¤¹è·¯å¾„
        """
        if not os.path.exists(history_folder):
            print(f"âŒ URLå†å²æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {history_folder}")
            return
        
        urls = []
        url_metadata = []
        
        # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for filename in os.listdir(history_folder):
            if filename.endswith('.txt'):
                # æ–‡ä»¶åå³ä¸ºæ”»å‡»ç±»å‹
                attack_type = filename.replace('.txt', '')
                file_path = os.path.join(history_folder, filename)
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        url = line.strip()
                        if url:
                            urls.append(url)
                            url_metadata.append({
                                'type': 'url_case',
                                'url': url,
                                'label': attack_type,  # 'normal', 'sqli', 'xss', etc.
                                'metadata': {}
                            })
        
        if not urls:
            print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•URLè®°å½•")
            return
        
        # ç”Ÿæˆembeddings
        print(f"ğŸ”„ æ­£åœ¨ä¸º {len(urls)} ä¸ªURLç”Ÿæˆå‘é‡...")
        embeddings = self.model.encode(urls, show_progress_bar=True)
        embeddings = np.array(embeddings).astype('float32')
        
        # å½’ä¸€åŒ–
        faiss.normalize_L2(embeddings)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        if self.index is None:
            self.index = faiss.IndexFlatIP(self.dimension)
        
        self.index.add(embeddings)
        self.metadata.extend(url_metadata)
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(urls)} ä¸ªURLæ¡ˆä¾‹")        

        
    def add_knowledge_documents(self, chunks_folder: str):
        """
        æ·»åŠ çŸ¥è¯†åº“æ–‡æ¡£åˆ°å‘é‡åº“
        
        Args:
            chunks_folder: chunksæ–‡ä»¶å¤¹è·¯å¾„
        """
        if not os.path.exists(chunks_folder):
            print(f"âŒ Chunksæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {chunks_folder}")
            return
        
        texts = []
        chunk_metadata = []
        
        # è¯»å–æ‰€æœ‰chunkæ–‡ä»¶
        for filename in os.listdir(chunks_folder):
            if filename.endswith('.txt'):
                file_path = os.path.join(chunks_folder, filename)
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                texts.append(content)
                chunk_metadata.append({
                    'type': 'knowledge',  # æ ‡è®°ä¸ºçŸ¥è¯†åº“æ–‡æ¡£
                    'attack_id': filename.replace('.txt', ''),
                    'source': filename
                })
        
        if not texts:
            print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•chunkæ–‡ä»¶")
            return
        
        # ç”Ÿæˆembeddings
        embeddings = self.model.encode(texts, show_progress_bar=True)
        embeddings = np.array(embeddings).astype('float32')
        
        # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(embeddings)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        if self.index is None:
            self.index = faiss.IndexFlatIP(self.dimension)
        
        self.index.add(embeddings)
        self.metadata.extend(chunk_metadata)
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(texts)} ä¸ªçŸ¥è¯†åº“æ–‡æ¡£")
    def search_in_url_cases_only(self, query_text: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """
        åªåœ¨URLæ¡ˆä¾‹ä¸­æ£€ç´¢
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            List[Tuple[int, float]]: (ç´¢å¼•, ç›¸ä¼¼åº¦åˆ†æ•°) åˆ—è¡¨
        """
        if not self.index:
            return []
        
        # 1. æ‰¾å‡ºæ‰€æœ‰URLæ¡ˆä¾‹çš„ç´¢å¼•
        url_indices = [i for i, m in enumerate(self.metadata) if m.get('type') == 'url_case']
        
        if not url_indices:
            return []
        
        # 2. å¯¹æŸ¥è¯¢æ–‡æœ¬ç¼–ç 
        query_embedding = self.encode([query_text])[0:1]
        
        # 3. è·å–æ‰€æœ‰å‘é‡
        all_vectors = self.index.reconstruct_n(0, self.index.ntotal)
        
        # 4. åªå–URLæ¡ˆä¾‹çš„å‘é‡
        url_vectors = np.array([all_vectors[i] for i in url_indices])
        
        # 5. è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(query_embedding, url_vectors.T)[0]
        
        # 6. æ’åºå¹¶å–top k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # 7. è¿”å›åŸå§‹ç´¢å¼•å’Œç›¸ä¼¼åº¦
        results = []
        for idx in top_indices:
            original_idx = url_indices[idx]
            score = float(similarities[idx])
            results.append((original_idx, score))
        
        return results
    
    # âœ¨ æ–°å¢æ–¹æ³•2ï¼šåªåœ¨çŸ¥è¯†åº“æ–‡æ¡£ä¸­æ£€ç´¢
    def search_in_knowledge_only(self, query_text: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """
        åªåœ¨çŸ¥è¯†åº“æ–‡æ¡£ä¸­æ£€ç´¢
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            List[Tuple[int, float]]: (ç´¢å¼•, ç›¸ä¼¼åº¦åˆ†æ•°) åˆ—è¡¨
        """
        if not self.index:
            return []
        
        # 1. æ‰¾å‡ºæ‰€æœ‰çŸ¥è¯†åº“æ–‡æ¡£çš„ç´¢å¼•
        knowledge_indices = [i for i, m in enumerate(self.metadata) if m.get('type') == 'knowledge']
        
        if not knowledge_indices:
            return []
        
        # 2. å¯¹æŸ¥è¯¢æ–‡æœ¬ç¼–ç 
        query_embedding = self.encode([query_text])[0:1]
        
        # 3. è·å–æ‰€æœ‰å‘é‡
        all_vectors = self.index.reconstruct_n(0, self.index.ntotal)
        
        # 4. åªå–çŸ¥è¯†åº“æ–‡æ¡£çš„å‘é‡
        knowledge_vectors = np.array([all_vectors[i] for i in knowledge_indices])
        
        # 5. è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(query_embedding, knowledge_vectors.T)[0]
        
        # 6. æ’åºå¹¶å–top k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # 7. è¿”å›åŸå§‹ç´¢å¼•å’Œç›¸ä¼¼åº¦
        results = []
        for idx in top_indices:
            original_idx = knowledge_indices[idx]
            score = float(similarities[idx])
            results.append((original_idx, score))
        
        return results