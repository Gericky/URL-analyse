"""
ç»“æœç»Ÿè®¡åˆ†ææ¨¡å— - è´Ÿè´£ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µæ£€æµ‹ç»“æœçš„ç»Ÿè®¡å’Œè¯„ä¼°
"""
import json
import os
from typing import List, Dict
from time import perf_counter
class ResultStatistics:
    """ç»“æœç»Ÿè®¡åˆ†æå™¨"""
    
    def __init__(self, all_results: List[Dict], output_config: Dict):
        """
        åˆå§‹åŒ–ç»Ÿè®¡åˆ†æå™¨
        
        Args:
            all_results: æ‰€æœ‰æ£€æµ‹ç»“æœåˆ—è¡¨
            output_config: è¾“å‡ºé…ç½®å­—å…¸
        """
        self.all_results = all_results
        self.output_config = output_config
        self.output_dir = output_config['dir']
        
        # åˆ†ç±»ç»“æœ
        self.normal_results = [r for r in all_results if r['predicted'] == "0"]
        self.anomalous_results = [r for r in all_results if r['predicted'] == "1"]
        
        # æŒ‰çœŸå®æ ‡ç­¾åˆ†ç±»
        self.true_normal_results = [r for r in all_results if r['true_label'] == "0"]
        self.true_attack_results = [r for r in all_results if r['true_label'] == "1"]
        
        # æ··æ·†çŸ©é˜µ
        self.tp = sum(1 for r in all_results if r['true_label'] == "1" and r['predicted'] == "1")
        self.tn = sum(1 for r in all_results if r['true_label'] == "0" and r['predicted'] == "0")
        self.fp = sum(1 for r in all_results if r['true_label'] == "0" and r['predicted'] == "1")
        self.fn = sum(1 for r in all_results if r['true_label'] == "1" and r['predicted'] == "0")
        
        # æ£€æµ‹æ–¹æ³•ç»Ÿè®¡
        self.rule_normal_count = sum(1 for r in all_results if r.get('detection_method') == 'rule_normal')
        self.rule_anomalous_count = sum(1 for r in all_results if r.get('detection_method') == 'rule_anomalous')
        self.model_count = sum(1 for r in all_results if r.get('detection_method') == 'model')
        
        # æŒ‰æ£€æµ‹æ–¹æ³•åˆ†ç±»ç»“æœ
        self.rule_results = [r for r in all_results if r.get('detection_method') in ['rule_normal', 'rule_anomalous']]
        self.model_results = [r for r in all_results if r.get('detection_method') == 'model']
        
        # æ•°æ®é›† + æ£€æµ‹æ–¹æ³•äº¤å‰ç»Ÿè®¡
        # æ­£å¸¸æ•°æ®é›† (true_label == "0")
        self.normal_by_rule = [r for r in self.true_normal_results if r.get('detection_method') in ['rule_normal', 'rule_anomalous']]
        self.normal_by_model = [r for r in self.true_normal_results if r.get('detection_method') == 'model']
        
        # æ”»å‡»æ•°æ®é›† (true_label == "1")
        self.attack_by_rule = [r for r in self.true_attack_results if r.get('detection_method') in ['rule_normal', 'rule_anomalous']]
        self.attack_by_model = [r for r in self.true_attack_results if r.get('detection_method') == 'model']
        
        # é”™è¯¯åˆ†æ
        self.fp_results = [r for r in all_results if r['true_label'] == "0" and r['predicted'] == "1"]
        self.fn_results = [r for r in all_results if r['true_label'] == "1" and r['predicted'] == "0"]
        # âœ¨ æ–°å¢ï¼šæ—¶é•¿ç»Ÿè®¡
        self.rule_normal_time = sum(r.get('elapsed_time_sec', 0) for r in all_results if r.get('detection_method') == 'rule_normal')
        self.rule_anomalous_time = sum(r.get('elapsed_time_sec', 0) for r in all_results if r.get('detection_method') == 'rule_anomalous')
        self.model_time = sum(r.get('elapsed_time_sec', 0) for r in all_results if r.get('detection_method') == 'model')
        
        self.total_rule_time = self.rule_normal_time + self.rule_anomalous_time
        self.total_model_time = self.model_time
        # âœ¨ æ–°å¢ï¼šè¯¦ç»†è§„åˆ™ç»Ÿè®¡
        self.rule_statistics = self._calculate_rule_statistics()
        
        # âœ¨ æ–°å¢ï¼šæŒ‰æ£€æµ‹æ–¹æ³•åˆ†ç±»çš„é”™è¯¯æ¡ˆä¾‹
        self.fp_by_rule = [r for r in self.fp_results if r.get('detection_method') in ['rule_normal', 'rule_anomalous']]
        self.fp_by_model = [r for r in self.fp_results if r.get('detection_method') == 'model']
        
        self.fn_by_rule = [r for r in self.fn_results if r.get('detection_method') in ['rule_normal', 'rule_anomalous']]
        self.fn_by_model = [r for r in self.fn_results if r.get('detection_method') == 'model']        
       
        # âœ¨ æ–°å¢ï¼šRAGæ£€æµ‹ç»Ÿè®¡
        self.rag_similarity_count = sum(1 for r in all_results if r.get('detection_method') == 'rag_similarity')
        self.model_with_rag_count = sum(1 for r in all_results if r.get('detection_method') == 'model_with_rag')
        
        # âœ¨ æ›´æ–°æ¨¡å‹ç»Ÿè®¡ï¼ˆåŒºåˆ†æ˜¯å¦ä½¿ç”¨RAGï¼‰
        self.model_pure_count = sum(1 for r in all_results if r.get('detection_method') == 'model')

    def _calculate_rule_statistics(self) -> Dict:
        """
        ç»Ÿè®¡æ¯æ¡è§„åˆ™çš„ä½¿ç”¨æƒ…å†µ
        
        Returns:
            dict: {
                "rule_id": {
                    "rule_name": str,
                    "attack_type": str,
                    "total_matched": int,
                    "correct": int,
                    "false_positive": int,
                    "false_negative": int,
                    "accuracy": float,
                    "avg_time_ms": float
                }
            }
        """
        rule_stats = {}
        
        for result in self.all_results:
            # åªå¤„ç†è§„åˆ™æ£€æµ‹çš„ç»“æœ
            if result.get('detection_method') not in ['rule_normal', 'rule_anomalous']:
                continue
            
            matched_rules = result.get('rule_matched', [])
            if not matched_rules:
                continue
            
            # é€šå¸¸æ¯ä¸ªURLåªåŒ¹é…ä¸€æ¡è§„åˆ™(ä¼˜å…ˆçº§æœ€é«˜çš„)
            for rule in matched_rules:
                rule_id = rule.get('rule_id', 'unknown')
                
                if rule_id not in rule_stats:
                    rule_stats[rule_id] = {
                        'rule_name': rule.get('rule_name', 'Unknown'),
                        'attack_type': rule.get('attack_type', 'unknown'),
                        'severity': rule.get('severity', 'unknown'),
                        'total_matched': 0,
                        'correct': 0,
                        'false_positive': 0,  # è§„åˆ™åˆ¤å¼‚å¸¸,å®é™…æ­£å¸¸
                        'false_negative': 0,   # è§„åˆ™åˆ¤æ­£å¸¸,å®é™…å¼‚å¸¸
                        'total_time': 0.0,
                        'times': []
                    }
                
                # ç»Ÿè®¡ä½¿ç”¨æ¬¡æ•°
                rule_stats[rule_id]['total_matched'] += 1
                
                # è®°å½•æ—¶é—´
                elapsed = result.get('elapsed_time_sec', 0)
                rule_stats[rule_id]['total_time'] += elapsed
                rule_stats[rule_id]['times'].append(elapsed)
                
                # åˆ¤æ–­æ­£ç¡®æ€§
                predicted = result.get('predicted')
                true_label = result.get('true_label')
                
                if predicted == true_label:
                    rule_stats[rule_id]['correct'] += 1
                else:
                    # è¯¯æŠ¥: åˆ¤ä¸ºå¼‚å¸¸(1),å®é™…æ­£å¸¸(0)
                    if predicted == "1" and true_label == "0":
                        rule_stats[rule_id]['false_positive'] += 1
                    # æ¼æŠ¥: åˆ¤ä¸ºæ­£å¸¸(0),å®é™…å¼‚å¸¸(1)
                    elif predicted == "0" and true_label == "1":
                        rule_stats[rule_id]['false_negative'] += 1
        
        # è®¡ç®—å‡†ç¡®ç‡å’Œå¹³å‡æ—¶é—´
        for rule_id, stats in rule_stats.items():
            total = stats['total_matched']
            if total > 0:
                stats['accuracy'] = stats['correct'] / total
                stats['avg_time_ms'] = (stats['total_time'] / total) * 1000
            else:
                stats['accuracy'] = 0.0
                stats['avg_time_ms'] = 0.0
        
        return rule_stats

    def print_stage1_basic_statistics(self, elapsed_time: float):
        """
        æ‰“å°ç¬¬ä¸€é˜¶æ®µåŸºç¡€ç»Ÿè®¡
        
        Args:
            elapsed_time: ç¬¬ä¸€é˜¶æ®µç”¨æ—¶ï¼ˆç§’ï¼‰
        """
        total = len(self.all_results)
        if total == 0:
            print(f"\n{'='*60}")
            print(f"âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰æ£€æµ‹ç»“æœ")
            print(f"{'='*60}\n")
            return
        
        # è®¡ç®—å®é™…æ£€æµ‹æ€»è€—æ—¶ï¼ˆè§„åˆ™ + æ¨¡å‹ï¼‰
        actual_detection_time = self.total_rule_time + self.total_model_time
        overhead_time = elapsed_time - actual_detection_time
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µåŸºç¡€ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"â±ï¸  æ€»è¿è¡Œæ—¶é—´: {elapsed_time:.2f} ç§’")
        print(f"   â”œâ”€ å®é™…æ£€æµ‹è€—æ—¶: {actual_detection_time:.4f} ç§’ ({actual_detection_time/elapsed_time*100:.1f}%)")
        print(f"   â”‚  â”œâ”€ è§„åˆ™æ£€æµ‹: {self.total_rule_time:.4f} ç§’")
        print(f"   â”‚  â””â”€ æ¨¡å‹æ£€æµ‹: {self.total_model_time:.4f} ç§’")
        print(f"   â””â”€ å…¶ä»–å¼€é”€: {overhead_time:.4f} ç§’ ({overhead_time/elapsed_time*100:.1f}%)")
        print(f"      (æ–‡ä»¶I/Oã€æ•°æ®å¤„ç†ç­‰)")
        print()
        print(f"ğŸ“Š æ€»URLæ•°: {total}")
        print(f"   å¹³å‡æ¯URLæ€»è€—æ—¶: {elapsed_time/total*1000:.2f} æ¯«ç§’")
        print(f"   å¹³å‡æ¯URLæ£€æµ‹è€—æ—¶: {actual_detection_time/total*1000:.2f} æ¯«ç§’")
        print()
        print(f"ğŸ“‚ è¾“å…¥æ•°æ®é›†:")
        print(f"   æ­£å¸¸URLæ•°æ®é›†: {len(self.true_normal_results)} æ¡")
        print(f"   æ”»å‡»URLæ•°æ®é›†: {len(self.true_attack_results)} æ¡")
        print()
        print(f"ğŸ¯ æ£€æµ‹ç»“æœ:")
        print(f"   åˆ¤å®šä¸ºæ­£å¸¸: {len(self.normal_results)} æ¡")
        print(f"   åˆ¤å®šä¸ºå¼‚å¸¸: {len(self.anomalous_results)} æ¡")
        print(f"{'='*60}")
    
    def calculate_metrics(self) -> Dict:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Returns:
            dict: åŒ…å«å„é¡¹è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        total = len(self.all_results)
        
        metrics = {
            'total': total,
            'tp': self.tp,
            'tn': self.tn,
            'fp': self.fp,
            'fn': self.fn,
            'accuracy': 0.0,
            'recall': 0.0,
            'precision': 0.0,
            'f1_score': 0.0,
            'fpr': 0.0,
            'fnr': 0.0
        }
        
        # å‡†ç¡®ç‡
        if total > 0:
            metrics['accuracy'] = (self.tp + self.tn) / total * 100
        
        # å¬å›ç‡
        if (self.tp + self.fn) > 0:
            metrics['recall'] = self.tp / (self.tp + self.fn) * 100
        
        # ç²¾ç¡®ç‡
        if (self.tp + self.fp) > 0:
            metrics['precision'] = self.tp / (self.tp + self.fp) * 100
        
        # F1åˆ†æ•°
        if (metrics['precision'] + metrics['recall']) > 0:
            metrics['f1_score'] = 2 * metrics['precision'] * metrics['recall'] / (metrics['precision'] + metrics['recall'])
        
        # è¯¯æŠ¥ç‡
        if (self.fp + self.tn) > 0:
            metrics['fpr'] = self.fp / (self.fp + self.tn) * 100
        
        # æ¼æŠ¥ç‡
        if (self.fn + self.tp) > 0:
            metrics['fnr'] = self.fn / (self.fn + self.tp) * 100
        
        return metrics
    
    def print_confusion_matrix(self):
        """æ‰“å°æ··æ·†çŸ©é˜µ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ··æ·†çŸ©é˜µ (Confusion Matrix)")
        print("=" * 60)
        print(f"{'':15} | é¢„æµ‹:æ­£å¸¸(0) | é¢„æµ‹:æ”»å‡»(1) | åˆè®¡")
        print("-" * 60)
        print(f"çœŸå®:æ­£å¸¸(0)  |    TN={self.tn:3d}     |    FP={self.fp:3d}     | {self.tn+self.fp:3d}")
        print(f"çœŸå®:æ”»å‡»(1)  |    FN={self.fn:3d}     |    TP={self.tp:3d}     | {self.fn+self.tp:3d}")
        print("-" * 60)
        print(f"åˆè®¡          |      {self.tn+self.fn:3d}      |      {self.fp+self.tp:3d}      | {len(self.all_results):3d}")
        print("=" * 60)
        print("\nè¯´æ˜:")
        print("  TP (True Positive):  æ­£ç¡®è¯†åˆ«ä¸ºæ”»å‡»")
        print("  TN (True Negative):  æ­£ç¡®è¯†åˆ«ä¸ºæ­£å¸¸")
        print("  FP (False Positive): è¯¯æŠ¥ - æ­£å¸¸URLè¢«åˆ¤å®šä¸ºæ”»å‡»")
        print("  FN (False Negative): æ¼æŠ¥ - æ”»å‡»URLè¢«åˆ¤å®šä¸ºæ­£å¸¸")
    
    def print_metrics(self):
        """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
        metrics = self.calculate_metrics()
        
        print("\n" + "=" * 60)
        print("ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡ (Evaluation Metrics)")
        print("=" * 60)
        print(f"âœ… å‡†ç¡®ç‡ (Accuracy):   {metrics['accuracy']:.2f}%")
        print(f"   = (TP + TN) / Total = ({self.tp} + {self.tn}) / {metrics['total']}")
        print(f"   å«ä¹‰: æ‰€æœ‰é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹")
        print()
        print(f"ğŸ¯ å¬å›ç‡ (Recall):     {metrics['recall']:.2f}%")
        print(f"   = TP / (TP + FN) = {self.tp} / ({self.tp} + {self.fn})")
        print(f"   å«ä¹‰: åœ¨æ‰€æœ‰çœŸå®æ”»å‡»ä¸­,æˆåŠŸè¯†åˆ«å‡ºçš„æ¯”ä¾‹")
        print(f"   (ä¹Ÿå«çœŸæ­£ç‡ TPR,è¶Šé«˜è¶Šå¥½,è¡¨ç¤ºä¸æ¼æ‰æ”»å‡»)")
        print()
        print(f"ğŸ” ç²¾ç¡®ç‡ (Precision):  {metrics['precision']:.2f}%")
        print(f"   = TP / (TP + FP) = {self.tp} / ({self.tp} + {self.fp})")
        print(f"   å«ä¹‰: åœ¨é¢„æµ‹ä¸ºæ”»å‡»çš„æ ·æœ¬ä¸­,çœŸæ­£æ˜¯æ”»å‡»çš„æ¯”ä¾‹")
        print(f"   (è¶Šé«˜è¶Šå¥½,è¡¨ç¤ºä¸è¯¯æŠ¥æ­£å¸¸URL)")
        print()
        print(f"âš–ï¸  F1åˆ†æ•° (F1-Score):   {metrics['f1_score']:.2f}%")
        print(f"   = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)")
        print(f"   å«ä¹‰: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡,ç»¼åˆè¯„ä»·æŒ‡æ ‡")
        print()
        print(f"âš ï¸  è¯¯æŠ¥ç‡ (FPR):       {metrics['fpr']:.2f}%")
        print(f"   = FP / (FP + TN) = {self.fp} / ({self.fp} + {self.tn})")
        print(f"   å«ä¹‰: æ­£å¸¸URLè¢«è¯¯åˆ¤ä¸ºæ”»å‡»çš„æ¯”ä¾‹ (è¶Šä½è¶Šå¥½)")
        print()
        print(f"âŒ æ¼æŠ¥ç‡ (FNR):       {metrics['fnr']:.2f}%")
        print(f"   = FN / (FN + TP) = {self.fn} / ({self.fn} + {self.tp})")
        print(f"   å«ä¹‰: æ”»å‡»URLè¢«æ¼åˆ¤ä¸ºæ­£å¸¸çš„æ¯”ä¾‹ (è¶Šä½è¶Šå¥½)")
        print("=" * 60)
    
    def print_detection_method_statistics(self):
        """æ‰“å°æ£€æµ‹æ–¹æ³•ç»Ÿè®¡ï¼ˆåŒ…å«RAGä¿¡æ¯ï¼‰"""
        total = len(self.all_results)
        
        if total == 0:
            print("\n" + "=" * 60)
            print("ğŸ”§ æ£€æµ‹æ–¹æ³•ç»Ÿè®¡")
            print("=" * 60)
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹ç»“æœå¯ä¾›ç»Ÿè®¡")
            print("=" * 60)
            return
        
        print("\n" + "=" * 60)
        print("ğŸ”§ æ£€æµ‹æ–¹æ³•ç»Ÿè®¡ï¼ˆæ•°é‡ + æ—¶é•¿ï¼‰")
        print("=" * 60)
        
        # è§„åˆ™æ£€æµ‹ç»Ÿè®¡
        total_rule_count = self.rule_normal_count + self.rule_anomalous_count
        print(f"\nğŸ” è§„åˆ™å¼•æ“æ£€æµ‹:")
        print(f"   â”œâ”€ æ€»åŒ¹é…æ•°: {total_rule_count} æ¡ ({total_rule_count/total*100:.1f}%)")
        print(f"   â”œâ”€ æ€»è€—æ—¶: {self.total_rule_time:.4f} ç§’")
        
        if total_rule_count > 0:
            avg_rule_time = self.total_rule_time / total_rule_count
            print(f"   â”œâ”€ å¹³å‡è€—æ—¶: {avg_rule_time*1000:.4f} æ¯«ç§’/æ¡")
            print(f"   â”‚")
            print(f"   â”œâ”€ åˆ¤å®šä¸ºæ­£å¸¸: {self.rule_normal_count} æ¡")
            if self.rule_normal_count > 0:
                print(f"   â”‚  â”œâ”€ è€—æ—¶: {self.rule_normal_time:.4f} ç§’")
                print(f"   â”‚  â””â”€ å¹³å‡: {self.rule_normal_time/self.rule_normal_count*1000:.4f} æ¯«ç§’/æ¡")
            print(f"   â”‚")
            print(f"   â””â”€ åˆ¤å®šä¸ºå¼‚å¸¸: {self.rule_anomalous_count} æ¡")
            if self.rule_anomalous_count > 0:
                print(f"      â”œâ”€ è€—æ—¶: {self.rule_anomalous_time:.4f} ç§’")
                print(f"      â””â”€ å¹³å‡: {self.rule_anomalous_time/self.rule_anomalous_count*1000:.4f} æ¯«ç§’/æ¡")
        
        # âœ¨ æ–°å¢ï¼šRAGç›¸ä¼¼åº¦æ£€æµ‹ç»Ÿè®¡
        if self.rag_similarity_count > 0:
            rag_time = sum(r.get('elapsed_time_sec', 0) for r in self.all_results if r.get('detection_method') == 'rag_similarity')
            print(f"\nğŸ” RAGç›¸ä¼¼åº¦æ£€æµ‹:")
            print(f"   â”œâ”€ æ£€æµ‹æ•°é‡: {self.rag_similarity_count} æ¡ ({self.rag_similarity_count/total*100:.1f}%)")
            print(f"   â”œâ”€ æ€»è€—æ—¶: {rag_time:.4f} ç§’")
            if self.rag_similarity_count > 0:
                print(f"   â””â”€ å¹³å‡è€—æ—¶: {rag_time/self.rag_similarity_count*1000:.4f} æ¯«ç§’/æ¡")
        
        # æ¨¡å‹æ£€æµ‹ç»Ÿè®¡ï¼ˆåŒºåˆ†æ˜¯å¦ä½¿ç”¨RAGï¼‰
        print(f"\nğŸ¤– æ¨¡å‹æ¨ç†æ£€æµ‹:")
        print(f"   â”œâ”€ æ£€æµ‹æ•°é‡: {self.model_count} æ¡ ({self.model_count/total*100:.1f}%)")
        print(f"   â”‚  â”œâ”€ RAGå¢å¼º: {self.model_with_rag_count} æ¡")
        print(f"   â”‚  â””â”€ çº¯æ¨¡å‹: {self.model_pure_count} æ¡")
        print(f"   â”œâ”€ æ€»è€—æ—¶: {self.total_model_time:.4f} ç§’")
        if self.model_count > 0:
            avg_model_time = self.total_model_time / self.model_count
            print(f"   â””â”€ å¹³å‡è€—æ—¶: {avg_model_time*1000:.4f} æ¯«ç§’/æ¡")
        
        # æ•ˆç‡å¯¹æ¯”
        if total_rule_count > 0 and self.model_count > 0:
            avg_rule_time = self.total_rule_time / total_rule_count
            avg_model_time = self.total_model_time / self.model_count
            speedup = avg_model_time / avg_rule_time
            print(f"\nâš¡ æ•ˆç‡å¯¹æ¯”:")
            print(f"   â””â”€ è§„åˆ™æ¯”æ¨¡å‹å¿« {speedup:.2f}x")
        
        # æ•´ä½“ç»Ÿè®¡
        print(f"\nğŸ“Š æ•´ä½“å‘½ä¸­ç‡:")
        print(f"   â”œâ”€ è§„åˆ™å‘½ä¸­ç‡: {total_rule_count/total*100:.1f}%")
        if self.rag_similarity_count > 0:
            print(f"   â”œâ”€ RAGå‘½ä¸­ç‡: {self.rag_similarity_count/total*100:.1f}%")
        print(f"   â””â”€ æ¨¡å‹è°ƒç”¨ç‡: {self.model_count/total*100:.1f}%")
        
        print("=" * 60)
    
    def print_dataset_method_statistics(self):
        """æ‰“å°æ•°æ®é›† Ã— æ£€æµ‹æ–¹æ³•äº¤å‰ç»Ÿè®¡"""
        print("\n" + "=" * 60)
        print("ğŸ“‚ æ•°æ®é›† Ã— æ£€æµ‹æ–¹æ³•äº¤å‰ç»Ÿè®¡")
        print("=" * 60)
        
        # æ­£å¸¸æ•°æ®é›†ç»Ÿè®¡
        total_normal = len(self.true_normal_results)
        normal_rule_count = len(self.normal_by_rule)
        normal_model_count = len(self.normal_by_model)
        
        # æ­£å¸¸æ•°æ®é›†çš„æ­£ç¡®è¯†åˆ«æ•°
        normal_correct_by_rule = sum(1 for r in self.normal_by_rule if r['predicted'] == "0")
        normal_correct_by_model = sum(1 for r in self.normal_by_model if r['predicted'] == "0")
        
        print(f"\nğŸŸ¢ æ­£å¸¸URLæ•°æ®é›† (å…± {total_normal} æ¡):")
        print(f"   â”œâ”€ è§„åˆ™å¼•æ“å¤„ç†: {normal_rule_count:3d} æ¡ ({normal_rule_count/total_normal*100:.1f}%)")
        if normal_rule_count > 0:
            print(f"   â”‚  â”œâ”€ æ­£ç¡®è¯†åˆ«: {normal_correct_by_rule} æ¡")
            print(f"   â”‚  â”œâ”€ è¯¯æŠ¥(åˆ¤ä¸ºæ”»å‡»): {normal_rule_count - normal_correct_by_rule} æ¡")
            print(f"   â”‚  â””â”€ å‡†ç¡®ç‡: {normal_correct_by_rule/normal_rule_count*100:.2f}%")
        print(f"   â””â”€ æ¨¡å‹æ¨ç†å¤„ç†: {normal_model_count:3d} æ¡ ({normal_model_count/total_normal*100:.1f}%)")
        if normal_model_count > 0:
            print(f"      â”œâ”€ æ­£ç¡®è¯†åˆ«: {normal_correct_by_model} æ¡")
            print(f"      â”œâ”€ è¯¯æŠ¥(åˆ¤ä¸ºæ”»å‡»): {normal_model_count - normal_correct_by_model} æ¡")
            print(f"      â””â”€ å‡†ç¡®ç‡: {normal_correct_by_model/normal_model_count*100:.2f}%")
        
        # æ”»å‡»æ•°æ®é›†ç»Ÿè®¡
        total_attack = len(self.true_attack_results)
        attack_rule_count = len(self.attack_by_rule)
        attack_model_count = len(self.attack_by_model)
        
        # æ”»å‡»æ•°æ®é›†çš„æ­£ç¡®è¯†åˆ«æ•°
        attack_correct_by_rule = sum(1 for r in self.attack_by_rule if r['predicted'] == "1")
        attack_correct_by_model = sum(1 for r in self.attack_by_model if r['predicted'] == "1")
        
        print(f"\nğŸ”´ æ”»å‡»URLæ•°æ®é›† (å…± {total_attack} æ¡):")
        print(f"   â”œâ”€ è§„åˆ™å¼•æ“å¤„ç†: {attack_rule_count:3d} æ¡ ({attack_rule_count/total_attack*100:.1f}%)")
        if attack_rule_count > 0:
            print(f"   â”‚  â”œâ”€ æ­£ç¡®è¯†åˆ«: {attack_correct_by_rule} æ¡")
            print(f"   â”‚  â”œâ”€ æ¼æŠ¥(åˆ¤ä¸ºæ­£å¸¸): {attack_rule_count - attack_correct_by_rule} æ¡")
            print(f"   â”‚  â””â”€ å‡†ç¡®ç‡: {attack_correct_by_rule/attack_rule_count*100:.2f}%")
        print(f"   â””â”€ æ¨¡å‹æ¨ç†å¤„ç†: {attack_model_count:3d} æ¡ ({attack_model_count/total_attack*100:.1f}%)")
        if attack_model_count > 0:
            print(f"      â”œâ”€ æ­£ç¡®è¯†åˆ«: {attack_correct_by_model} æ¡")
            print(f"      â”œâ”€ æ¼æŠ¥(åˆ¤ä¸ºæ­£å¸¸): {attack_model_count - attack_correct_by_model} æ¡")
            print(f"      â””â”€ å‡†ç¡®ç‡: {attack_correct_by_model/attack_model_count*100:.2f}%")
        
        print("=" * 60)
    
    def print_method_performance_comparison(self):
        """æ‰“å°æ£€æµ‹æ–¹æ³•æ€§èƒ½å¯¹æ¯”"""
        print("\n" + "=" * 60)
        print("âš”ï¸  æ£€æµ‹æ–¹æ³•æ€§èƒ½å¯¹æ¯”")
        print("=" * 60)
        
        # è§„åˆ™å¼•æ“æ€§èƒ½
        rule_total = len(self.rule_results)
        if rule_total > 0:
            rule_tp = sum(1 for r in self.rule_results if r['true_label'] == "1" and r['predicted'] == "1")
            rule_tn = sum(1 for r in self.rule_results if r['true_label'] == "0" and r['predicted'] == "0")
            rule_fp = sum(1 for r in self.rule_results if r['true_label'] == "0" and r['predicted'] == "1")
            rule_fn = sum(1 for r in self.rule_results if r['true_label'] == "1" and r['predicted'] == "0")
            
            rule_accuracy = (rule_tp + rule_tn) / rule_total * 100
            rule_fpr = rule_fp / (rule_fp + rule_tn) * 100 if (rule_fp + rule_tn) > 0 else 0
            rule_fnr = rule_fn / (rule_fn + rule_tp) * 100 if (rule_fn + rule_tp) > 0 else 0
            
            print(f"\nğŸ“ è§„åˆ™å¼•æ“ (å¤„ç† {rule_total} æ¡):")
            print(f"   â”œâ”€ å‡†ç¡®ç‡: {rule_accuracy:.2f}%")
            print(f"   â”œâ”€ è¯¯æŠ¥ç‡: {rule_fpr:.2f}% ({rule_fp}/{rule_fp+rule_tn} æ­£å¸¸URLè¢«è¯¯åˆ¤)")
            print(f"   â”œâ”€ æ¼æŠ¥ç‡: {rule_fnr:.2f}% ({rule_fn}/{rule_fn+rule_tp} æ”»å‡»URLè¢«æ¼åˆ¤)")
            print(f"   â””â”€ æ··æ·†çŸ©é˜µ: TP={rule_tp}, TN={rule_tn}, FP={rule_fp}, FN={rule_fn}")
        else:
            print(f"\nğŸ“ è§„åˆ™å¼•æ“: æœªå¤„ç†ä»»ä½•URL")
        
        # æ¨¡å‹æ¨ç†æ€§èƒ½
        model_total = len(self.model_results)
        if model_total > 0:
            model_tp = sum(1 for r in self.model_results if r['true_label'] == "1" and r['predicted'] == "1")
            model_tn = sum(1 for r in self.model_results if r['true_label'] == "0" and r['predicted'] == "0")
            model_fp = sum(1 for r in self.model_results if r['true_label'] == "0" and r['predicted'] == "1")
            model_fn = sum(1 for r in self.model_results if r['true_label'] == "1" and r['predicted'] == "0")
            
            model_accuracy = (model_tp + model_tn) / model_total * 100
            model_fpr = model_fp / (model_fp + model_tn) * 100 if (model_fp + model_tn) > 0 else 0
            model_fnr = model_fn / (model_fn + model_tp) * 100 if (model_fn + model_tp) > 0 else 0
            
            print(f"\nğŸ¤– æ¨¡å‹æ¨ç† (å¤„ç† {model_total} æ¡):")
            print(f"   â”œâ”€ å‡†ç¡®ç‡: {model_accuracy:.2f}%")
            print(f"   â”œâ”€ è¯¯æŠ¥ç‡: {model_fpr:.2f}% ({model_fp}/{model_fp+model_tn} æ­£å¸¸URLè¢«è¯¯åˆ¤)")
            print(f"   â”œâ”€ æ¼æŠ¥ç‡: {model_fnr:.2f}% ({model_fn}/{model_fn+model_tp} æ”»å‡»URLè¢«æ¼åˆ¤)")
            print(f"   â””â”€ æ··æ·†çŸ©é˜µ: TP={model_tp}, TN={model_tn}, FP={model_fp}, FN={model_fn}")
        else:
            print(f"\nğŸ¤– æ¨¡å‹æ¨ç†: æœªå¤„ç†ä»»ä½•URL")
        
        print("=" * 60)
    
    def print_error_analysis(self, max_display: int = 3):
        """æ‰“å°é”™è¯¯åˆ†æ(å¢å¼ºç‰ˆ:æŒ‰æ£€æµ‹æ–¹æ³•åˆ†ç±»)"""
        print("\n" + "=" * 60)
        print("âŒ é”™è¯¯åˆ†æ")
        print("=" * 60)
        
        # è¯¯æŠ¥åˆ†æ
        print(f"\nğŸ”´ è¯¯æŠ¥ (False Positives): {len(self.fp_results)} æ¡")
        if len(self.fp_results) > 0:
            print(f"   â”œâ”€ è§„åˆ™è¯¯æŠ¥: {len(self.fp_by_rule)} æ¡ ({len(self.fp_by_rule)/len(self.fp_results)*100:.1f}%)")
            print(f"   â””â”€ æ¨¡å‹è¯¯æŠ¥: {len(self.fp_by_model)} æ¡ ({len(self.fp_by_model)/len(self.fp_results)*100:.1f}%)")
            
            # æ˜¾ç¤ºè§„åˆ™è¯¯æŠ¥è¯¦æƒ…
            if len(self.fp_by_rule) > 0:
                print(f"\n   ğŸ“Œ è§„åˆ™è¯¯æŠ¥è¯¦æƒ… (å‰{min(max_display, len(self.fp_by_rule))}æ¡):")
                for i, case in enumerate(self.fp_by_rule[:max_display], 1):
                    rule_name = case.get('rule_matched', [{}])[0].get('rule_name', 'Unknown')
                    print(f"      {i}. è§„åˆ™: {rule_name}")
                    print(f"         URL: {case['url'][:80]}...")
                    print(f"         åŸå› : {case.get('reason', 'N/A')}")
            
            # æ˜¾ç¤ºæ¨¡å‹è¯¯æŠ¥è¯¦æƒ…
            if len(self.fp_by_model) > 0:
                print(f"\n   ğŸ“Œ æ¨¡å‹è¯¯æŠ¥è¯¦æƒ… (å‰{min(max_display, len(self.fp_by_model))}æ¡):")
                for i, case in enumerate(self.fp_by_model[:max_display], 1):
                    print(f"      {i}. åˆ¤å®š: {case.get('attack_type', 'unknown')}")
                    print(f"         URL: {case['url'][:80]}...")
        
        # æ¼æŠ¥åˆ†æ
        print(f"\nğŸ”µ æ¼æŠ¥ (False Negatives): {len(self.fn_results)} æ¡")
        if len(self.fn_results) > 0:
            print(f"   â”œâ”€ è§„åˆ™æ¼æŠ¥: {len(self.fn_by_rule)} æ¡ ({len(self.fn_by_rule)/len(self.fn_results)*100:.1f}%)")
            print(f"   â””â”€ æ¨¡å‹æ¼æŠ¥: {len(self.fn_by_model)} æ¡ ({len(self.fn_by_model)/len(self.fn_results)*100:.1f}%)")
            
            # æ˜¾ç¤ºè§„åˆ™æ¼æŠ¥è¯¦æƒ…
            if len(self.fn_by_rule) > 0:
                print(f"\n   ğŸ“Œ è§„åˆ™æ¼æŠ¥è¯¦æƒ… (å‰{min(max_display, len(self.fn_by_rule))}æ¡):")
                for i, case in enumerate(self.fn_by_rule[:max_display], 1):
                    rule_name = case.get('rule_matched', [{}])[0].get('rule_name', 'Normal Pattern')
                    print(f"      {i}. è§„åˆ™: {rule_name}")
                    print(f"         URL: {case['url'][:80]}...")
            
            # æ˜¾ç¤ºæ¨¡å‹æ¼æŠ¥è¯¦æƒ…
            if len(self.fn_by_model) > 0:
                print(f"\n   ğŸ“Œ æ¨¡å‹æ¼æŠ¥è¯¦æƒ… (å‰{min(max_display, len(self.fn_by_model))}æ¡):")
                for i, case in enumerate(self.fn_by_model[:max_display], 1):
                    print(f"      {i}. URL: {case['url'][:80]}...")
        
        print("=" * 60)
    
    def print_attack_type_distribution(self):
        """æ‰“å°æ”»å‡»ç±»å‹åˆ†å¸ƒ"""
        if not self.anomalous_results:
            return
        
        attack_types = {}
        for result in self.anomalous_results:
            attack_type = result.get('attack_type', 'unknown')
            attack_types[attack_type] = attack_types.get(attack_type, 0) + 1
        
        print("\n" + "=" * 60)
        print("ğŸ¯ å¼‚å¸¸URLæ”»å‡»ç±»å‹åˆ†å¸ƒ")
        print("=" * 60)
        total_anomalous = len(self.anomalous_results)
        for attack_type, count in sorted(attack_types.items(), key=lambda x: x[1], reverse=True):
            percentage = count / total_anomalous * 100
            print(f"  {attack_type:20s}: {count:3d} æ¡ ({percentage:.1f}%)")
        print("=" * 60)
    
    def print_rule_detailed_statistics(self):
        """æ‰“å°æ¯æ¡è§„åˆ™çš„è¯¦ç»†ä½¿ç”¨ç»Ÿè®¡"""
        if not self.rule_statistics:
            print("\n" + "=" * 60)
            print("ğŸ“‹ è§„åˆ™è¯¦ç»†ç»Ÿè®¡")
            print("=" * 60)
            print("âš ï¸  æ²¡æœ‰è§„åˆ™åŒ¹é…è®°å½•")
            print("=" * 60)
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ è§„åˆ™è¯¦ç»†ç»Ÿè®¡ (æŒ‰åŒ¹é…æ¬¡æ•°æ’åº)")
        print("=" * 60)
        
        # æŒ‰åŒ¹é…æ¬¡æ•°æ’åº
        sorted_rules = sorted(
            self.rule_statistics.items(),
            key=lambda x: x[1]['total_matched'],
            reverse=True
        )
        
        for rule_id, stats in sorted_rules:
            print(f"\nğŸ” è§„åˆ™ID: {rule_id}")
            print(f"   åç§°: {stats['rule_name']}")
            print(f"   æ”»å‡»ç±»å‹: {stats['attack_type']}")
            print(f"   ä¸¥é‡çº§åˆ«: {stats['severity']}")
            print(f"   åŒ¹é…æ¬¡æ•°: {stats['total_matched']}")
            print(f"   æ­£ç¡®åˆ¤æ–­: {stats['correct']} ({stats['accuracy']*100:.1f}%)")
            print(f"   è¯¯æŠ¥ (FP): {stats['false_positive']}")
            print(f"   æ¼æŠ¥ (FN): {stats['false_negative']}")
            print(f"   å¹³å‡è€—æ—¶: {stats['avg_time_ms']:.4f} æ¯«ç§’")
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š è§„åˆ™æ€»æ•°: {len(self.rule_statistics)}")
        total_matched = sum(s['total_matched'] for s in self.rule_statistics.values())
        total_correct = sum(s['correct'] for s in self.rule_statistics.values())
        print(f"ğŸ“Š æ€»åŒ¹é…æ¬¡æ•°: {total_matched}")
        print(f"âœ… æ€»æ­£ç¡®æ¬¡æ•°: {total_correct} ({total_correct/total_matched*100:.1f}%)")
        print("=" * 60)

    def save_results(self):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¿å­˜ç¬¬ä¸€é˜¶æ®µæ‰€æœ‰ç»“æœ
        stage1_all_file = os.path.join(
            self.output_dir, 
            self.output_config.get('stage1_all', 'stage1_realtime_all.json')
        )
        with open(stage1_all_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
        metrics = self.calculate_metrics()
        
        # âœ¨ æ–°å¢ï¼šæ—¶é•¿ç»Ÿè®¡ä¿¡æ¯
        total_rule_count = self.rule_normal_count + self.rule_anomalous_count
        
        # æ‰©å±•æŒ‡æ ‡ï¼šæ·»åŠ æ—¶é•¿ç»Ÿè®¡
        extended_metrics = {
            **metrics,
            'timing_statistics': {
                'rule_engine': {
                    'total_count': total_rule_count,
                    'total_time_sec': round(self.total_rule_time, 6),
                    'avg_time_sec': round(self.total_rule_time / total_rule_count, 6) if total_rule_count > 0 else 0,
                    'avg_time_ms': round(self.total_rule_time / total_rule_count * 1000, 4) if total_rule_count > 0 else 0,
                    'normal_rules': {
                        'count': self.rule_normal_count,
                        'total_time_sec': round(self.rule_normal_time, 6),
                        'avg_time_ms': round(self.rule_normal_time / self.rule_normal_count * 1000, 4) if self.rule_normal_count > 0 else 0
                    },
                    'anomalous_rules': {
                        'count': self.rule_anomalous_count,
                        'total_time_sec': round(self.rule_anomalous_time, 6),
                        'avg_time_ms': round(self.rule_anomalous_time / self.rule_anomalous_count * 1000, 4) if self.rule_anomalous_count > 0 else 0
                    }
                },
                'model_inference': {
                    'total_count': self.model_count,
                    'total_time_sec': round(self.total_model_time, 6),
                    'avg_time_sec': round(self.total_model_time / self.model_count, 6) if self.model_count > 0 else 0,
                    'avg_time_ms': round(self.total_model_time / self.model_count * 1000, 4) if self.model_count > 0 else 0
                },
                'speedup': round(
                    (self.total_model_time / self.model_count) / (self.total_rule_time / total_rule_count),
                    2
                ) if (total_rule_count > 0 and self.model_count > 0) else 0
            },
            'dataset_statistics': {
                'normal_dataset': {
                    'total': len(self.true_normal_results),
                    'by_rule': len(self.normal_by_rule),
                    'by_model': len(self.normal_by_model),
                    'correct_by_rule': sum(1 for r in self.normal_by_rule if r['predicted'] == "0"),
                    'correct_by_model': sum(1 for r in self.normal_by_model if r['predicted'] == "0")
                },
                'attack_dataset': {
                    'total': len(self.true_attack_results),
                    'by_rule': len(self.attack_by_rule),
                    'by_model': len(self.attack_by_model),
                    'correct_by_rule': sum(1 for r in self.attack_by_rule if r['predicted'] == "1"),
                    'correct_by_model': sum(1 for r in self.attack_by_model if r['predicted'] == "1")
                }
            },
            'method_performance': {
                'rule_engine': self._calculate_method_metrics(self.rule_results),
                'model_inference': self._calculate_method_metrics(self.model_results)
            }
        }
        
        metrics_file = os.path.join(self.output_dir, 'stage1_metrics.json')
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(extended_metrics, f, ensure_ascii=False, indent=2)
        
        # âœ¨ æ–°å¢ï¼šä¿å­˜è§„åˆ™è¯¦ç»†ç»Ÿè®¡
        if self.rule_statistics:
            rule_stats_file = os.path.join(self.output_dir, 'rule_statistics.json')
            # ç§»é™¤timesåˆ—è¡¨,åªä¿ç•™æ±‡æ€»æ•°æ®
            clean_stats = {}
            for rule_id, stats in self.rule_statistics.items():
                clean_stats[rule_id] = {
                    k: v for k, v in stats.items() if k != 'times'
                }
            with open(rule_stats_file, 'w', encoding='utf-8') as f:
                json.dump(clean_stats, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è§„åˆ™ç»Ÿè®¡å·²ä¿å­˜: {rule_stats_file}")
        
        # âœ¨ æ–°å¢ï¼šä¿å­˜æŒ‰æ£€æµ‹æ–¹æ³•åˆ†ç±»çš„è¯¯æŠ¥
        fp_by_method_file = os.path.join(self.output_dir, 'stage1_false_positives_by_method.json')
        fp_by_method = {
            "summary": {
                "total": len(self.fp_results),
                "by_rule": len(self.fp_by_rule),
                "by_model": len(self.fp_by_model)
            },
            "rule_based_fp": self.fp_by_rule,
            "model_based_fp": self.fp_by_model
        }
        with open(fp_by_method_file, 'w', encoding='utf-8') as f:
            json.dump(fp_by_method, f, ensure_ascii=False, indent=2)
        
        # âœ¨ æ–°å¢ï¼šä¿å­˜æŒ‰æ£€æµ‹æ–¹æ³•åˆ†ç±»çš„æ¼æŠ¥
        fn_by_method_file = os.path.join(self.output_dir, 'stage1_false_negatives_by_method.json')
        fn_by_method = {
            "summary": {
                "total": len(self.fn_results),
                "by_rule": len(self.fn_by_rule),
                "by_model": len(self.fn_by_model)
            },
            "rule_based_fn": self.fn_by_rule,
            "model_based_fn": self.fn_by_model
        }
        with open(fn_by_method_file, 'w', encoding='utf-8') as f:
            json.dump(fn_by_method, f, ensure_ascii=False, indent=2)
        
        # âœ… ä¿å­˜åŸæœ‰çš„è¯¯æŠ¥/æ¼æŠ¥æ–‡ä»¶ï¼ˆä¿®å¤ï¼šå®šä¹‰å˜é‡ï¼‰
        fp_file = os.path.join(self.output_dir, 'stage1_false_positives.json')
        fp_data = {
            "total_count": len(self.fp_results),
            "by_rule": len(self.fp_by_rule),
            "by_model": len(self.fp_by_model),
            "cases": self.fp_results
        }
        with open(fp_file, 'w', encoding='utf-8') as f:
            json.dump(fp_data, f, ensure_ascii=False, indent=2)
        
        fn_file = os.path.join(self.output_dir, 'stage1_false_negatives.json')
        fn_data = {
            "total_count": len(self.fn_results),
            "by_rule": len(self.fn_by_rule),
            "by_model": len(self.fn_by_model),
            "cases": self.fn_results
        }
        with open(fn_file, 'w', encoding='utf-8') as f:
            json.dump(fn_data, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°ä¿å­˜ä¿¡æ¯
        print(f"\nğŸ’¾ ç¬¬ä¸€é˜¶æ®µç»“æœå·²ä¿å­˜: {stage1_all_file}")
        print(f"ğŸ’¾ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {metrics_file}")
        print(f"ğŸ’¾ è¯¯æŠ¥åˆ†ç±»å·²ä¿å­˜: {fp_by_method_file}")
        print(f"ğŸ’¾ æ¼æŠ¥åˆ†ç±»å·²ä¿å­˜: {fn_by_method_file}")
        print(f"ğŸ’¾ è¯¯æŠ¥æ¡ˆä¾‹å·²ä¿å­˜: {fp_file} (å…± {len(self.fp_results)} æ¡)")
        print(f"ğŸ’¾ æ¼æŠ¥æ¡ˆä¾‹å·²ä¿å­˜: {fn_file} (å…± {len(self.fn_results)} æ¡)")
    
    def _calculate_method_metrics(self, results: List[Dict]) -> Dict:
        """è®¡ç®—ç‰¹å®šæ–¹æ³•çš„æŒ‡æ ‡"""
        if not results:
            return {
                'total': 0,
                'accuracy': 0.0,
                'fpr': 0.0,
                'fnr': 0.0
            }
        
        tp = sum(1 for r in results if r['true_label'] == "1" and r['predicted'] == "1")
        tn = sum(1 for r in results if r['true_label'] == "0" and r['predicted'] == "0")
        fp = sum(1 for r in results if r['true_label'] == "0" and r['predicted'] == "1")
        fn = sum(1 for r in results if r['true_label'] == "1" and r['predicted'] == "0")
        
        total = len(results)
        accuracy = (tp + tn) / total * 100 if total > 0 else 0
        fpr = fp / (fp + tn) * 100 if (fp + tn) > 0 else 0
        fnr = fn / (fn + tp) * 100 if (fn + tp) > 0 else 0
        
        return {
            'total': total,
            'tp': tp,
            'tn': tn,
            'fp': fp,
            'fn': fn,
            'accuracy': round(accuracy, 2),
            'fpr': round(fpr, 2),
            'fnr': round(fnr, 2)
        }
    
    def generate_full_report(self, stage1_elapsed: float):
        """
        ç”Ÿæˆå®Œæ•´ç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            stage1_elapsed: ç¬¬ä¸€é˜¶æ®µç”¨æ—¶ï¼ˆç§’ï¼‰
        """
        if len(self.all_results) == 0:
            print(f"\n{'='*60}")
            print(f"âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰æ£€æµ‹ç»“æœ")
            print(f"{'='*60}")
            print(f"è¯·æ£€æŸ¥:")
            print(f"  1. æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            print(f"  2. æ•°æ®æ–‡ä»¶æ˜¯å¦ä¸ºç©º")
            print(f"  3. æ–‡ä»¶è·¯å¾„é…ç½®æ˜¯å¦æ­£ç¡®")
            print(f"{'='*60}\n")
            return
        
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        # æ‰“å°ç¬¬ä¸€é˜¶æ®µåŸºç¡€ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ€»æ•°ã€ç”¨æ—¶ç­‰ï¼‰
        self.print_stage1_basic_statistics(stage1_elapsed)
        # æ‰“å°æ··æ·†çŸ©é˜µï¼ˆTP/TN/FP/FNåˆ†å¸ƒï¼‰
        self.print_confusion_matrix()
        # æ‰“å°è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€å¬å›ç‡ç­‰ï¼‰
        self.print_metrics()
        # æ‰“å°æ£€æµ‹æ–¹æ³•ç»Ÿè®¡ï¼ˆè§„åˆ™/æ¨¡å‹æ•°é‡ä¸æ—¶é•¿ï¼‰
        self.print_detection_method_statistics()
        
        # âœ¨ æ–°å¢ï¼šæ‰“å°æ¯æ¡è§„åˆ™çš„è¯¦ç»†ä½¿ç”¨ç»Ÿè®¡
        self.print_rule_detailed_statistics()
        
        # æ‰“å°æ•°æ®é›†ä¸æ£€æµ‹æ–¹æ³•äº¤å‰ç»Ÿè®¡
        self.print_dataset_method_statistics()
        # æ‰“å°æ£€æµ‹æ–¹æ³•æ€§èƒ½å¯¹æ¯”ï¼ˆè§„åˆ™ä¸æ¨¡å‹ï¼‰
        self.print_method_performance_comparison()
        # æ‰“å°é”™è¯¯åˆ†æï¼ˆè¯¯æŠ¥/æ¼æŠ¥æ¡ˆä¾‹ï¼‰
        self.print_error_analysis()
        # æ‰“å°å¼‚å¸¸URLæ”»å‡»ç±»å‹åˆ†å¸ƒ
        self.print_attack_type_distribution()
        # ä¿å­˜æ‰€æœ‰ç»Ÿè®¡ç»“æœåˆ°æ–‡ä»¶
        self.save_results()
    
    

def analyze_results(all_results: List[Dict], output_config: Dict, stage1_elapsed: float):
    """
    åˆ†æç¬¬ä¸€é˜¶æ®µç»“æœçš„ä¾¿æ·å‡½æ•°
    
    Args:
        all_results: æ‰€æœ‰æ£€æµ‹ç»“æœåˆ—è¡¨
        output_config: è¾“å‡ºé…ç½®å­—å…¸
        stage1_elapsed: ç¬¬ä¸€é˜¶æ®µç”¨æ—¶ï¼ˆç§’ï¼‰
    """
    analyzer = ResultStatistics(all_results, output_config)
    analyzer.generate_full_report(stage1_elapsed)


def print_stage2_statistics(elapsed_time: float, output_file: str, deep_results: List[Dict]):
    """
    æ‰“å°ç¬¬äºŒé˜¶æ®µæ·±åº¦åˆ†æç»Ÿè®¡
    
    Args:
        elapsed_time: ç¬¬äºŒé˜¶æ®µç”¨æ—¶ï¼ˆç§’ï¼‰
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        deep_results: æ·±åº¦åˆ†æç»“æœåˆ—è¡¨
    """
    if len(deep_results) == 0:
        print(f"\n{'='*60}")
        print(f"âš ï¸  ç¬¬äºŒé˜¶æ®µï¼šæ²¡æœ‰éœ€è¦æ·±åº¦åˆ†æçš„URL")
        print(f"{'='*60}\n")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ç¬¬äºŒé˜¶æ®µç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {elapsed_time:.2f} ç§’")
    print(f"ğŸ“ˆ å¹³å‡æ¯URLç”¨æ—¶: {elapsed_time/len(deep_results):.2f} ç§’")
    print(f"ğŸ“Š æ·±åº¦åˆ†æURLæ•°: {len(deep_results)}")
    print(f"ğŸ’¾ æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_file}")
    print(f"{'='*60}")


def print_two_stage_summary(stage1_elapsed: float, stage2_elapsed: float):

    """
    æ‰“å°ä¸¤é˜¶æ®µæ£€æµ‹æ€»ç»“
    
    Args:
        stage1_elapsed: ç¬¬ä¸€é˜¶æ®µç”¨æ—¶ï¼ˆç§’ï¼‰
        stage2_elapsed: ç¬¬äºŒé˜¶æ®µç”¨æ—¶ï¼ˆç§’ï¼‰
    """
    total_elapsed = stage1_elapsed + stage2_elapsed
    print(f"\n{'='*60}")
    print(f"ğŸ¯ ä¸¤é˜¶æ®µæ£€æµ‹å®Œæˆ")
    print(f"{'='*60}")
    print(f"â±ï¸  ç¬¬ä¸€é˜¶æ®µç”¨æ—¶: {stage1_elapsed:.2f} ç§’")
    print(f"â±ï¸  ç¬¬äºŒé˜¶æ®µç”¨æ—¶: {stage2_elapsed:.2f} ç§’")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {total_elapsed:.2f} ç§’")
    print(f"{'='*60}\n")

def print_file_time_statistics(file_times):
    """æ‰“å°å„æ–‡ä»¶å¤„ç†æ—¶é•¿ç»Ÿè®¡"""
    if not file_times:
        return
    
    print(f"\n{'='*70}")
    print(f"ğŸ“ å„æ–‡ä»¶å¤„ç†æ—¶é•¿ç»Ÿè®¡")
    print(f"{'='*70}")
    
    total_time = 0
    total_records = 0
    
    for filename, elapsed, count in file_times:
        avg_time = elapsed / count if count > 0 else 0
        print(f"ğŸ“„ {filename:30s}")
        print(f"   â±ï¸  å¤„ç†æ—¶é•¿: {elapsed:8.2f} ç§’")
        print(f"   ğŸ“Š æ•°æ®é‡:   {count:8d} æ¡")
        print(f"   âš¡ å¹³å‡è€—æ—¶: {avg_time:8.4f} ç§’/æ¡")
        print()
        
        total_time += elapsed
        total_records += count
    
    print(f"{'-'*70}")
    print(f"âœ… æ€»è€—æ—¶:   {total_time:8.2f} ç§’")
    print(f"ğŸ“ˆ æ€»æ•°æ®é‡: {total_records:8d} æ¡")
    avg_total = total_time / total_records if total_records > 0 else 0
    print(f"âš¡ æ•´ä½“å¹³å‡: {avg_total:8.4f} ç§’/æ¡")
    print(f"{'='*70}\n")